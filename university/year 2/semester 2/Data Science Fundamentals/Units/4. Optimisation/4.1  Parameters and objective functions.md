#dsf
# What is Optimisation?

**Optimisation** is the process of adjusting things to make them better. In comp sci we want to do this via an algorithm.

# Parameters and Objective Function

- **Parameters**, the things we can adjust, which might be a scalar or vector or other array of values.
	- Denoted by $θ$.
	- The parameters exist in a parameter space – the set of all possible configurations of parameters denoted $Θ$.
	
- **The objective function**, a function that maps the parameters onto a single numerical measure of how good the configuration is $L(θ)$

Writing this mathematically, this is the `argmin` (the argument that produces the minimum value) of the objective function:

$$θ^∗ = argmin_{θ∈Θ} L(θ)$$
- $θ^∗$ is the configuration that we want to find; the one for which the objective function is lowest. 
- $Θ$ is the set of all possible configurations that $Θ$ could take on, e.g. $R^N$ 

Most optimisation problems have one more component:

- **constraints**: the limitations on the parameters. 
	- This defines a region of the parameter space that is feasible, the **feasible set** or **feasible region**. For example, a synthesizer has knobs with a physical range, say 0 - 10; it isn't possible to turn it to 11.

## Minimising Differences

As in this example, it is common to have express problems in a form where the objective function is a **distance between an output and a reference is measured**. Not every objective function has this form, but many do.

---

**DOUBLE BACK ON THIS SECTION, GOT NO CLUE WHAT THIS MEANS:***

That is, we have some function y′ = f (~x; θ) that produces an output from an input ~x governed by a set of parameters θ, and we measure the difference between the output and some reference y (e.g. using a vector norm)

$$ L(θ) = ‖y′ − y‖ = ‖ f (~x; θ) − y‖$$

---

This is very common in approximation problems, where we want to find a function that approximates a set of measured observations. This is the core problem of machine learning.

Note that the notation f (~x; θ) just means that the output of f depends both on some (vector) input ~x and on a parameter vector θ.

## Evaluating the Objective Function

It may be expensive to evaluate the objective function.

- the computation might take a long time (invert a 10000x10000 matrix)
- or it might require a real-world experiment to be performed (do the users like the new app layout?);
- or it might be dangerous (which wire on the bomb should I cut next?);
-  or it might require data that must be bought and paid for (literally expensive)

In all cases it will take some computational power to evaluate the objective function and therefore will have a time cost.

This means that a good optimisation algorithm will find the optimal configuration of parameters with few queries. To do this, there must be a mathematical **structure** which can help guide the search. 

## Throwing a Stone Example

Every optimisation problem has two parts:

- **Parameters** the things that can be adjusted.
- **Objective Function** which measures how good a particular set of parameters are.

For example, if I wanted to optimise how far I could throw a stone, I might be able to adjust the throwing angle. This is the parameter I could tweak (just one parameter $θ = [α]$, in this case)

```python
import numpy as np 

def L(theta): 
	# L *must* depend on the parameters I can adjust 
	# in this case, there is only one; the throw angle 
	
	# pull out the angle, convert from degrees to radians 
	angle = np.radians(theta[0])
	
	# initial throw position x=0m, y=1m 
	pos = np.array([0.0, 1.0]) 
	
	# initial throw velocity, depends on angle 
	vel = np.array([np.cos(angle), np.sin(angle)]) 
	
	posns = [] 
	# simulate throwing the ball, until it hits the ground
	while pos[1] >= 0:
		pos += vel
		vel[1] -= 0.005 # gravity 
		vel -= 0.01 * vel # air resistance 
		posns.append(list(pos))
	
	posns = np.array(posns) 
	
	plt.gca().plot(posns[:, 0], posns[:, 1], "k", lw=0.1)
	
	# return how far our throw went 
	# remember that we want to minimise 
	# our objective function, so this value 
	# must get *lower* as our throw gets longer 
	
	return -np.abs(pos[0])
```

```python
import scipy.optimize 

fig, ax = plt.subplots() 
ax.set_xlabel("Horizontal location (m)") 
ax.set_ylabel("Vertical location (m)")

# use a built in optimiser to solve this 
res = scipy.optimize.minimize(L, [65.0], method="nelder-mead")
print(res)
```

![[Screenshot 2025-04-15 at 14.35.11.png]]

# Focus: Continuous Optimisation in Real Vector Spaces

This course will focus on optimisation of continuous problems in $R^n$

$$θ ∈ Rl^n = [θ_1, θ_2, … , θ_n]$$
This is the problem of searching a continuous vector space to find the point where L(θ) is smallest.

Some optimisation algorithms are iterative, in that they generate successively better approximations to a solution. Other methods are direct, like linear least squares (which we'll briefly discuss), and involving finding a minimum exactly in one step. We will focus primarily on iterative, approximate optimisation in this course.

# Constrained Optimisation

If a problem has constraints on the parameters beyond purely minimising the objective function then the problem is **constrained optimisation.** 

A constrained optimisation might be written in terms of an equality constraint:

$$θ^∗ = argmin_{θ∈Θ} L(θ) \text{ subject to } c(θ) = 0$$

where c(θ) is a function that represents the constraints.

## Common Constraint Types

- A **box constraint** is a simple kind of constraint, and is just a requirement that θ lie within a box inside $R^n$; 
- A **convex constraint** is another simple kind of constraint, where the constraint is a collection of inequalities on a convex sum of the parameters $θ$.
- **Unconstrained optimisation** does not apply any constraints to the parameters, and any parameter configuration in the search space is possible

## Constrained Optimisation

Use an optimisation algorithm that supports hard constraints inherently. This is straightforward for certain kinds of optimisation, but trickier for general optimisation.

- **Pros**
	- Guarantees that solution will satisfy constraints.
	- May be able to use constraints to speed up optimisation
- **Cons**
	- May be less efficient than unconstrained optimisation.
	- Fewer algorithms to optimise.

## Soft Constraints

pply penalties to the objective function to "discourage" solutions that violate the constraints. This is particularly appropriate if the constraints really are soft (it doesn't perhaps matter if the maximum airfoil length is 1.5m or 1.6m, but it can't be 10m). 

- **Pros** 
	- any optimiser can be used 
	- can deal with soft constraints sensibly 

- **Cons**: 
	- may not respect important constraints, particularly if they are very sharp 
	- can be hard to formulate constraints as penalties 
	- cannot take advantage of efficient search in constrained regions of space

# Relaxation of Objective Functions

It can be much harder to solve discrete optimisation and constrained optimisation problems efficiently; some algorithms try and find similar continuous or unconstrained optimisation problems to solve instead.

This is called **relaxation**; a **relaxed** version of the problem is solved instead of the original hard optimisation problem.

## Penalisation

**Penalisation** refers to terms which augment an objective function to minimise some other property of the solution, typically to approximate constrained optimisation.

### Penalty Functions

A penalty function is just a term added to an objective function which will disfavour "bad solutions".

We can return to the stone throwing example, and extend our model. Say I can control the angle of a stone throw; perhaps I can also control how hard I throw it. But there is a maximum limit to my strength. This is a constraint (an inequality constraint, which limits the maximum value of the strength parameter).

```python
def penalty(strength):
	# start applying constraint just before we hit
	# the hard limit 
	cutoff = max_strength * 0.90 
	# increase loss rapidly as we excced max strength 
	return np.where(
		strength > cutoff,
		(cutoff - strength) ** 2 * 500,
		0
	)

# super simple 
# add the penalty to the objective function 
def throw_penalised(theta):
	loss = throw(theta) + penalty(theta[1])
	return loss

fig = plt.figure() 
ax = fig.add_subplot(1, 1, 1) 
ax.set_xlabel("Horizontal location (m)") 
ax.set_ylabel("Vertical location (m)")

# no constraints this time
res = scipy.optimize.minimize(
	throw_penalised,
	[65.0, 0.1],
	method="nelder-mead"
)

print(
	"Angle {angle:.1f} degrees, distance={m:.1f}m".format(
		angle=res.x[0],
		m=np.abs(res.fun)
	)
)
```