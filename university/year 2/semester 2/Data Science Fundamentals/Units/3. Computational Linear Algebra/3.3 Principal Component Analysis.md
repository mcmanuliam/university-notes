#dsf
# Edge-weighted Graphs

If some of the connections in a graph are stronger than others, imagine if they are connect through bigger roads (just a weighted graph) we can show the adjacency matrix as:

![[Screenshot 2025-04-09 at 14.21.48.png]]

Then display as:

```python
A = [
	[0.0, 2.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0],
	[0.0, 0.0, 0.0, 0.25, 3.0, 0.0, 0.0, 0.0],
	[0.0, 2.0, 0.0, 4.0 , 0.0, 0.0, 0.0, 0.0],
	[0.0, 0.0, 0.0, 0.0 , 0.0, 0.5, 2.0, 0.5],
	[0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],
	[0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],
	[0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],
	[3.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0]
]

# plot the adjacency matrix A as an image plot
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(1,1,1)
img = ax.imshow(A)
ax.set_xticks(np.arange(8))
ax.set_xticklabels("ABCDEFGH")
ax.set_yticks(np.arange(8))
ax.set_yticklabels("ABCDEFGH")
fig.colorbar(img)
ax.set_title("Adjacency matrix as an image plot");
```

![[Screenshot 2025-04-09 at 14.23.08.png]]

We saw the covariance matrix Σ in the previous Unit. It tells us how much correlation there is between the variables in a data set.

We can plot a representation of the covariance matrix as an ellipse which aligns with the distribution of the data points. Uncorrelated data is represented by a circle, whereas strongly correlated data is represented by a long thin ellipse. The eigenvectors of the covariance matrix, scaled by their eigenvalues, form the principal axes of the ellipse.

# Matrix Powers (exponentiation)
# Decomposition of the Covariance Matrix into It's Eigenvectors and Eignenvalues

The eigenvectors of the covariance matrix are called the **principal components**, and they tell us the directions in which the data varies most. This is an incredibly useful thing to be able to do, particularly with high-dimesional data where the variables may be correlated in complicated ways.

```python
# Construct a matrix x of 200 random data points (x_1, x_2), where x_1 and x_2 , are correlated

x = np.random.normal(0, 1, (200, 2)) @ np.array([[0.05, 0.4], [-0.9, 1.0]])

# plot the dataset
fig = plt.figure(figsize=(8, 8))

ax = fig.add_subplot(1, 1, 1)
ax.scatter(x[:, 0], x[:, 1], c='C0', label="Original data", s=3, alpha=0.8)
ax.set_xlim(-3, 3) 
ax.set_ylim(-4, 4)

plt.xlabel('$x_1$')
plt.ylabel('$x_2$');
```

# Reconstruction of the Covariance Matrix from Its Eigenvectors and Eigenvalues

Since we are able to decompose the covariance matrix into its constituent eigenvectors and eigenvalues, we must also be able to use these constituent parts to reconstruct the covariance matrix. We can do this as follows:

Where $Q^T$ is the transposition of T.

![[Screenshot 2025-04-09 at 14.17.09.png]]

```python
for i in range(x.shape[0]):  
    for j in range(x.shape[1]):  
        if (y[i,j]):  
            z += x[i,j]**4  
        else:  
            z += np.sqrt(x[i,j])

def exampleFunc(x, y):
	if (y[i,j]):  
        z += x[i,j]**4  
    else:  
        z += np.sqrt(x[i,j])
```