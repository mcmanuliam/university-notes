#dsf
# Use of Matrices

We have seen that (real) vectors represent elements of a vector space as 1D arrays of real numbers (and implemented as `ndarrays` of floats).

Matrices represent **linear maps** as 2D arrays of reals; $R^{m*n}$

- Vectors represent "points in space"
- Matrices represent operations that do things to those points in space.
# Operations with Matrices

There are many thing we can do with matricesL

 - They can be added and subtracted $C = A + B$
	 - ($R^{n*m}$, $R^{n*m}$) -> $R^{n*m}$

- They can be scaled with a scalar $C = _SA$
	 - ($R^{n*m}$, R) -> $R^{n*m}$

- They can be transposed $B = A^T$, exchanging rows and columns
	- $R^{n*m}$ -> $R^{m*n}$

- They can be applied to vectors, this applies a matrix to a vector.
	- ($R^{n*m}$, $R^m$) -> $R^n$

- Can also be multiplied together
	- ![[Screenshot 2025-04-07 at 22.35.48.png]]
	- Length of columns for first matrix needs to match the number of rows in the second.

# Intro to Matrix Notation

We write matrixes with a capital letter:

![[Screenshot 2025-04-07 at 22.37.23.png]]

**Not enforced in code**.

A matrix with dimension $n * m$ has n rows and m columns (remember this order). Each element of the matrix A is written as $a_{ij}$ for ith row and jth column.

Matrices do match the 2D arrays we are familiar with but remember to distinguish these two.

# Matrices as Maps

We saw vectors as **points in space**. Matrices represent linear maps.

$A_x = f(x)$

This is equivalent to applying some function $f(x)$ to the vectors.

## Effect of Matrix Transform.

Specifically, a $n × m$ matrix A represents a function $f(x)$ taking `m` dimensional vectors to `n` dimensional vectors, $(R^m → R^n)$ such that all straight lines remain straight and all parallel lines remain parallel.

# Matrix Operations

There is an algebra of matrices; this is linear algebra. In particular, there is a concept of addition of matrices of equal size, which is simple element wise addition.

![[Screenshot 2025-04-08 at 14.43.20.png]]

Along with scalar multiplication cA, which multiplies each element by c.

![[Screenshot 2025-04-08 at 14.43.46.png]]

## Application to Vectors

We can apply a matrix to a vector. We write it as a product $A\overrightarrow{x}$, to mean the matrix A applied to the vector $\overrightarrow{x}$. This is equivalent to applying the function f (~x), where f is the corresponding function.

**All application of a matrix to a vector does is form a weighted sum of the elements of the vector. This is a linear combination (equivalent to a "weighted sum") of the components.**

In particular, we take each element of $\overrightarrow{x}, x_1, x_2, … , x_m$, multiply it with the corresponding column of A, and sum these columns together.

# Time Complexity of Multiplication

Matrix multiplication has, in the general case, of time complexity $O(pqr)$, or for multiplying two square matrices $O(n^3)$.

# Commutativity

The order of multiplication is important. Matrix multiplication does not commute; in general:

![[Screenshot 2025-04-08 at 15.41.21.png]]

# Anatomy of a Matrix

The diagonal entries of a matrix (Aii) are important "**landmarks**" in the structure of a matrix. Matrix elements are often referred to as being "diagonal" or "off-diagonal" terms.

Matrices which are all zero except for a single diagonal entry are called… diagonal matrices. They represent a transformation that is an independent scaling of each dimension. Such matrices transform cubes to cuboids (i.e. all angles remain unchanged, and no rotation occurs).

`np.diag(x)` will return a diagonal matrix for a given vector x:

```python
print_matrix("\\text{diag}(x)", np.diag([1,2,3,3,2,1]))
```

The anti-diagonal is the set of elements $A_{i[N−i]}$ for an NxN matrix, for example a 3x3 binary antidiagonal matrix

```python
# an anti-diagonal array is the diagonal of the flipped array
print_matrix("\\text{anti-diagonal}", np.fliplr(np.diag([1,2,3])))
```

## Identity

The identity matrix is denoted I and is a n square matrix, where all values are zero except 1 along the diagonal.

![[Screenshot 2025-04-08 at 15.48.37.png]]

The identity matrix has no effect when multiplied by another matrix or vector. (Obviously, it must be dimension compatible to be multiplied at all).

## Zero

The zero matrix is all zeros, and is defined for any matrix size m × n. It is written as 0. Multiplying any vector or matrix by the zero matrix results in a result consisting of all zeros. The 0 matrix maps all vectors onto the zero vector (the origin).

## Square

A matrix is square if it has size $n × n$. Square matrices are important, because they apply transformations within a vector space; a mapping from n dimensional space to n dimensional space; a map from $R^n → R^n$. 

They represent functions mapping one domain to itself. Square matrices are the only ones that: * have an inverse * have determinants * have an **eigen decomposition**.

which are all ideas we will see in the following unit.

## Triangular

A square matrix is triangular if it has non-zero elements only above (upper triangular) or below the diagonal (lower triangular), inclusive of the diagonal.

![[Screenshot 2025-04-08 at 15.51.34.png]]