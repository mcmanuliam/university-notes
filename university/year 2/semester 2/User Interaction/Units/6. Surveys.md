#ui 

Surveys are everywhere! They are used to make inferences about an entire population by gathering information from a smaller subset (sample) of the population.

- They are fast, low cost and have broad reach
- Can be done using various platforms and modalities
	- Online vs. Offline

- Big difference between quick and dirty surveys that are properly planned, constructed and analysed. A well planned survey can elicit useful insights while a poor one can be a waste of time.
# What Are They Good For?

- Measuring Attitude over time both accurately and objectively
- Understanding **Intent** or motivation at a given time
- Quantify **Task Success** reliably
- Collect open-ended **User Experience** feedback
- Understanding **User Characteristics** to better serve their needs
- Understanding **Interactions with Technologies**
- Measuring **Awareness** of people of existing technology
- Making **Comparisons** users' attitudes, perceptions, and experiences across user segments, time, geographies, and competing applications.

# When to Avoid?

- **Precise Behaviour**: Log data can often give more accurate info
- **Underlying Motivations**: People often unsure/can't explain why they prefer one thing over another. Better use ethnography or contextual inquiry.
- **Usability Evaluations**: Surveys inappropriate for testing specific usability and understanding of tools and applications. Interview methods (usability studies) are better.

# Pitfalls

- Surveys also need to consider experimental design and confounding factors.
- Multiple dependent variables in a single question.
- Low completion rates, resulting from repetitive questions and poor usability and design. 
- Noisy data from bad questionnaire design (e.g., vague or ambiguous questions, biased questions)

# How to Design a Good Survey?

## 1. Research Goals

- Identify research goals and useful constructs
	- Only crucial constructs: too many 'nice to know' questions will make a survey too long increasing drop-out rate
	
- Cognitive pretesting: Are respondents interpreting constructs as intended?

## 2. Population and Sampling

- Survey respondents need to be recruited from the Sampling Frame 
- To ensure that the intended population is represented, you might need some inclusion criteria
	- For example, only respondents with over 20 hours gameplay can participate 

- Refer to earlier lectures for details about population, samples.

## 3. Questionnaire Design

- Poor design introduces bias and measurement error (deviation of respondent's answers from their true values)

- Mainly 2 categories of questions
	- Open Ended
	- Close Ended

### Bias

Each question must be carefully checked for bias. Five common bias types:

- Satisficing (dishonesty, inattentive, or in a rush) 
- Acquiescence bias (tendency to agree) 
- Social desirability (viewed favourably) 
- Response order bias 
- Question order bias

**Satisficing** occurs when respondents use a suboptimal amount of cognitive effort to answer questions.

Fails to follow one of the 4 cognitive steps: comprehension of questions, retrieval of information from memory, judgement of the retrieved answer, mapping of the answer to the survey option.

Weak satisficing: pick an answer that is suboptimal 

Strong satisficing: pick an answer randomly 

Satisfying occurs when cognitive ability or motivation to answer is low; Or, the question difficulty is high increasing cognitive load.

**How to minimise satisfying?**
- Avoid complex questions and long questionnaires 
- Force choice by eliminating options such as "no opinion" 
- Avoid by offering even number of possible responses on scale 
- Avoid using same rating scale for a series of back-to-back questions 
- Ask to justify answer 
- Explain the importance of the survey to increase motivation

### Types of Questions to Avoid

- **Broad Questions** lack focus, provide noisy data and confuse respondents 
- **Leading questions** influence respondents to give a certain answer and add bias 
- Double-barrelled questions ask about multiple items while only allowing for a single response, resulting in less reliable and valid data. 
- **Recall questions** require the respondent to remember past attitudes and behaviours, leading to inaccurate and biased answers 
- **Prediction questions** ask respondents to anticipate future behaviours or attitudes, resulting in biased and inaccurate responses.

## 4. Review and Testing

Early review and evaluation of a survey removes any confusion, identifies disconnect between researcher's assumptions and how respondents will read, interpret, and answer questions.

**Cognitive pretesting**: A small set of potential respondents is invited to participate in an in-person interview where they are asked to take the survey while using the think-aloud protocol.

**Field testing**: Run a pilot to review and evaluate survey before public launch

## 5. Implementation and Launch

 After questions are finalised, the survey is ready to be fielded. 
 
 Respondents may be invited through e-mails to specifically named persons, intercept pop-up dialogs while using a product or a site (intercept survey), or links placed directly in an application.

## 6. Data Analysis and Reporting

After survey responses are collected, make sense of the data by following the steps below: 

1. Preparing and exploring data 
2. Analysing data 
3. Reporting insights and results

**Step 1**: Preparing data is important for removing duplicate responses, careless/inaccurate responses (speeders), straight-liners and other questionable patterns, missing data, outliers, inadequate open-ended responses etc.

**Step 2**: Analysis of close-ended question requires 

- Descriptive statistics (moments, dispersion, etc) 
- Inferential statistics (correlation, regression, clustering etc.) 
- Hypothesis testing

- 'Coding' - transform qualitative answers to quantitative metric 
- Prepare a full list of possible answers. Assign a 'code' (may be numbers or representative terms) to each answer 
- Assign each open-ended response to one of the 'codes' (rating) 
- Might require more than one raters while assigning the codes (normally inter-rater reliability). 
- Perform statistical analysis on the codes (e.g., frequency)

**Step 3**: Reporting results and insights

- Results of data analysis should be reported with the necessary statistical rigor (e.g., sample sizes, p -values, margins of error). 
- Report survey paradata (e.g., devices used, completion time, drop-off rates). 
- Describe original research goals and survey methodology. This explains the population being studied, sampling method, survey mode etc.
- Include screenshots of the actual survey questions and explain techniques used to evaluate data 
- Discuss how the respondents compare to the overall population.