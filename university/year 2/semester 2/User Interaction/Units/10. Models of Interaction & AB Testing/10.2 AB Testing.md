# Interesting Question: How Do You Test Models of Interaction Live?

Now introducing A/B Testing:

- Goes by many names but it's the same idea:
	-  Web Experiments
	- Control/Treatment
	- Randomised Experimental
	- Controlled Experiments

- Randomly split traffic among different app versions
- Collect metrics and analyse

# Some Design Elements that Can Make a a Significant Difference in Page Performance

- Heading: Position and prominence 
- Columns: Number of columns used on the page 
- Visual Elements: competing for attention 
- Whitespace: on a page, space to 'breathe'
- Photos: The age, sex and appearance of someone 
- Call to actions: position and colour 
- Testimonials: Position 
- Content type: text or as image

# Warning

- Doesn't tell us **why** one is more successful
	- Interviews, Quantitive methods.

- Main point: it is hard to assess the value of ideas 
	- Get the data by experimenting because data beats intuition

# Ramp-Up Approach

- To detect an effect, you need to expose a certain number of users to the treatment (based on power calculations)
- But: don't start an experiment at 50/50% 
	- That's too much risk, ramp up over a short period 
	- Start an experiment at 0.1% of user base (or other small value) 
	- Do simple analyses to make sure no egregious problems can be detected
	- Ramp up to a more significant percentage, and repeat until 50%

# Advantages of A/B Testing

- It tests for causal relationships, not just correlations
	- measure direct impact of change

- It reduces the effect of external factors 
	- e.g., history/seasonality impact A and B the same 

- Overcome poor intuition, especially with novel ideas 
	- All too often, the less data, the stronger the opinions 
	- So, get the data through experimentation

# Challenges with A/B Testing

- Organisation has to agree on OEC (Overall Evaluation Criterion) 
	- This is hard, but it provides a clear direction and project alignment

- Quantitative metrics may not explain why one is better or worse
	- No help for designers solve 
	- No guidance for next design iteration

- Primacy effect
	- cognitive bias where users tend to favor or remember the first piece of information, they encounter more than subsequent information
	- Changing the app or site may degrade the user experience (temporarily) even if the new design is better

- Multiple experiments
	- Statistical variance increases, harder to get statistically significant results 

- Consistency and contamination 
	- Assignment to A or B is usually cookie-based, but people may use multiple machines or erase cookies 
	- Be careful to do proper randomisation!