
Almost any advance computing application has some randomization or statistical:

- Network Security
- Cryptography
- Web Search
- Spam Filtering
- Social Networking Tools
- â€¦

# Probability in Computing Science

**Randomized**
- Cryptography and Security
- Fast Algorithms
- Simulations

**Probabilistic analysis of algorithms**

**Statistical Inference**
- Machine Learning, Deep Learning, Data Mining

All are based on the same (mostly discrete) probability theory principles and techniques.

# Before We Go onâ€¦

The following are heavily linked to Probabiltiy, if you're stuck it might be good to revisit some of these topics:

- [[3.3 Sets]]
- Cardinality of Sets (number of elements in a set)
- Venn Diagrams

# A Quick Overview of Probability - Outline

- Probability - Introduction & Definitions
- Axioms and Properties of Probabiltiy
- Conditional Probability

## First: What is a "Proposition"

It is a "declarative" sentence that is either `TRUE` or `FALSE` (but not both).

- More about this laterâ€¦

## Probability - Introduction

In the real world, we often do not know whether a given proposition is `TRUE` or `FALSE`.  Probability theory gives us a way to reason about propositions whose truth is uncertain.

**Probability theory deals with (random) experiments**
- These are processes or actions whose outcome cannot be predicted with certainty and may differ if the experiment is repeated.

**Examples:**
- Rolling a dice
	- Possible outcomes: `1, â€¦, 6`
	
- Drawing two cards from a shuffled pack of cards 
	- possible outcomes are pairs of cards: `{5, 6}, {J, A}`
	
 - Flipping a coin
	 - possible outcomes: `heads` and `tails`

***Probability theory aims at quantifying the uncertainty surrounding the possible outcomes of an experiment***

# Definitions

- **Experiment** (or trial): an occurrence with an uncertain outcome
- **Outcome**: the result of an experiment
- **Sample space**: the set Î© of all possible outcomes for the experiment
- **Event**: subset AâŠ†Î© of possible outcomes with some common property
- **Frequency**: how often an outcome occurs in a sequence of experiments

**Probability**: the probability of an event is the degree of certainty than an event will occur (use P[A] to denote probability of event A) 

- `P = 0` â†’ This event will never happen 
- `P = 1` â†’ This event will always happen 
- `P = 0.5` â†’ There is a 50% chance of this event happening

# Calculating Probability
If ð›º is a finite **sample space**â€¦ of equally likely **outcomes**, and A is an **event**, (that is, a subset of $\omega$), then the probability of A is

$$P(A) = \frac{A}\omega$$

![[Pasted image 20241209100615.png]]

## Examples

### Rolling a Single Die

- Sample space Î©=`{ 1, 2, 3, 4, 5, 6 }`
	- 6 possible outcomes all equally likely 
	
- probability of rolling an even number: $P(A) = \frac{|A|} {|\omega|}$
	- `A = {2, 4, 6}` 
	- `P[A] = 3/6 = 1/2`

-  probability of not rolling a 6
	- `A = {1,2,3,4,5}` 
	- `P[A] = 5/6`

### Rolling Two Dice

 - Sample space $\omega =\{ (i,j) | 1â‰¤iâ‰¤6 âˆ§ 1â‰¤jâ‰¤6 \}$
 
 - `6ï½¥6 = 36` possible outcomes (using the product rule) and all equally likely
 
 - probability that sum is even?

-  Why product and sum rules here? 
	- When computing probability we are essentially "counting" number of ways something happens (an event), that may be composed of: 
		- multiple separate tasks that together form that event, e.g. two dice being thrown â†’ product rule 
		- different ways the same event can happen (an event can happen like this OR that â†’ sum rule 
		
	- Thus product rule and sum rule find application here as well

- Thus, P(A) = ways both dice can be even + ways both dice can be add / total number of ways two dice can be thrown

- (Ways first dice can be even * ways second dice can be even) + (ways first dice can be odd * ways second dice can be odd) / total number of ways two dice can be thrown.

- $= (3 * 3) + (3 * 3)/36 = 1/2$

## A Quick Overview of Probability - Outline

- **Probability** - Introduction & Definitions
- **Axioms** and **Properties** of Probability
- Conditional Probability

# Axioms

There are three basic axioms of probability from which everything else can be derived. Given that:

- We have a sample space $\omega$ and events $A, A \subseteq \omega$
- The probability of the event A is denoted P[A] 
- Events A and B are mutually exclusive if $A \cap B = \emptyset$

**Axiom 1**: $0 \leq P[A] \leq 1$
**Axiom 2**: $P[\omega]=1$
**Axiom 3**: If `A` and `B` are mutually exclusive, then $P[A \cup B]=P[A]+P[B]$
- If A and B have no overlap, and are completely separate it can be determined that the probability of both being true is equal to $P[A]+P[B]$

![[Pasted image 20241209104416.png]]

***This is essentially the sum rule, redone for Probability***

## Two Dice Example in View of Axiom #3
**Axiom 3**: If `A` and `B` are mutually exclusive, then $P[A \cup B]=P[A]+P[B]$

- Sample space $\omega = \{(i, j) | 1 \leq i \leq 6 âˆ§ 1 \leq j \leq 6\}$
- Probability that sum is even using **Axiom #3**

$$P[\text{Both even}] + P[\text{Both odd}]$$
$$(3 * 3)/36 + (3 * 3)/36$$
$$1/2$$
## Properties from Axioms
Some important properties

- $P[\emptyset]=0$
- $P[A \cup B]=P[A] + P[B] - P[A \cap B]$ for any events A and B
	- ***This is essentially the "inclusion-exclusion" principle of counting.***
- $P[A]=1-P[\overline{A}]$ for any event A

![[Pasted image 20241209104921.png]]

# Conditional Probabilities
Probability of an event, given that another event happened.

- E.g. given than that a dice throw is even, what is the probability that it is a 4?

For events A and B, if $P[B] \gt 0$, then the conditional probability of A given B is defined by: $P[A|B]$

For events A and B, if $P[B]>0$, then the conditional probability of A given B is defined by: $P[A|B] = \frac{P[A \cap B]}{P[B]}$

***In simple ways this simplifies the set, we now know it's even now we are trying to find the probability of the intersection of even and 4.***

![[Pasted image 20241209105402.png]]

**Think of it this way: 

Given that you have been told that event B has already happened, **THAT** is now your Universe, and the size of set B (its cardinality) will now be in the denominator (rather than the size of the original Universe).

For the numerator, you need to count the number of events of interest inside your new Universe. These are those elements of A (the event of interest) that are also in B (which is now the new Universe). That is, |Aâˆ©B|. 

So: 
$P[A|B] = |Aâˆ©B| / |B|$ or equivalently
$P[A|B]=P[A \cap B] / P[B]$

If A and B are independent then $P[A|B] = P[A]$. This leads to a few funny ideas for questions:

- I have a car with four wheels, what's the odds when I roll a dice it's a 4

## Examples

**Example**: given than that a dice throw is even, what is the probability that it is a 4?

- A the value of the die equals 4 (1 in 6 chance so probability 1/6) 
- B the value of the die is even (3 in 6 chance so probability 1/2)
- Aâˆ©B the value of the die is 4 and even (which is the same as A)
-  $P[A|B] = P[Aâˆ©B]/P[B] = (1/6)/(1/2) = 1/3$
	- A|B means value is 4 given the value is even (1 in 3 chance, so probability 1/3)

# Bayes Rule

Conditional Probability:

$$A[A|B]=\frac{P[A \cap B]}{P[B]}$$

In some cases, we don't know $P[A \cap B]$
- That is, we do not have enough information about overlap between A and B

But we are often in the situation that we can compute $P[B|A]$
- In general the two values $P[A|B]$ and $P[B|A]$ can be completely different

Typically, this type of problem occurs where we:
- want to know the probability of some event given some evidence 
	- the likelihood I have a disease given that my blood test was positive 
	
- but we only know the probability of the evidence given the event 
	- if you have the disease, the blood test is positive 95% of the time

## Assume that

- 5% of incoming emails are spam (probability an email is spam: 0.05)
- 50% of spam emails contain the word 'beneficiary' (probability 0.5)
- 6% of all emails contain the word 'beneficiary' (probability 0.06)

What is the probability that an email is spam, given it contains the word 'beneficiary'.

- $P[A \cap B]$ not know
- But: $P[B|A]$ is know
- Express $P[A \cap B]$ in terms of $P[B|A]$

 $$P[B|A]=\frac{P[B \cap A]}{P[A]}$$
  $$But: P[B|A] \text{ is known}$$
   $$\text{Since } P[B \cap A] = P[A \cap B]$$
      $$P[B \cap A] = P[B|A]*P[A] \text{ -> replace in the conditional probability equation for P[A|B]}$$

Bayes' rule gives a consistent rule to take some prior belief and combine it with observed data to make a new prediction.

We often phrase this as some hypothesis H we want to know, given some data D we observe, and we can write Bayes' Rule as:

$$P[H|D] = \frac{P[D|H] P[H]}{P[D]}$$
in other words 
- To work out how likely a hypothesis is true given some data 
- but only know how likely the data is if the hypothesis was true 
- we can use Bayes' rule to solve the problem

# 3.1. Problem Set: Probability
- [ ] ðŸ”½ 

1. In roulette there is a wheel with 38 numbers of these 18 are red and 18 are black. The other two numbers are 0 and 00 which are neither red nor black. The probability that when the wheel is spun it lands an a particular number is 1/38. 

	- (a) What is the probability the wheel lands on a red number? 
		- $P(red) = \frac{18}{38}$
	
	- (b) What is the probability the wheel lands on a black number twice in a row? 
		-  $P(black-twice) = \frac{9}{49}$
	
	- (c) What is the probability the wheel lands on 0 or 00? 
		- $P(0or00)=\frac{2}{38}$
	
	- (d) What is the probability in five spins the wheel neither lands on 0 nor 00? 
		- /
	
	- (e) What is the probability the wheel lands on one of the first six integers on one spin, but does not land on any of them on the next spin?

		- /