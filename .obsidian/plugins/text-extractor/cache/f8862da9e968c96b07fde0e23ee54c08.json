{"path":"misc/Papermill AI Session/Slides/Font CLIP.pdf","text":"Font CLIP For a given font, search a dataset of freely available fonts for a similar one. TaskAnatomy of a Typography Typographies are complex objects, the results of dozens of design decisions compounded to create something incredibly unique. Capturing all of this complexity manually is essentially impossible. ● What makes two fonts “similar”? ● When can one font replace another without being too jarring? Luckily, we don’t have to answer these questions ● If our goal is to compute the “semantic” similarity between two images, we can’t rely on the input space (pixel values) directly ● The goal of representation learning is to train a neural network capable of converting an input image into a “latent” vector of values which encodes the necessary information for a downstream task ○ Note that this encoding doesn’t need to be understood by a human What is Representation Learning?Contrastive Learning ● A deep learning model is trained to translate inputs into “queries” and “keys” ● Queries and keys are vectors belonging to a shared latent space ● The model is trained such that a given query matches as close as possible to the corresponding key while matching alternative keys as little as possible Sim-CLR (Simple Contrastive Learning) ● A powerful general purpose image representation learning technique ● Given a batch of images, apply a randomised set of transformations twice to each image ● We now have two sets of images where image i in set A “matches” image i in set B ● Contrastive learning is used to train a convolutional neural network (CNN) which learns a representative vector space invariant of these transforms ● Applying this technique to font images was useful as a learning exercise but the results weren’t quite acceptable Contrastive Language-Image Pretraining (CLIP) ● Developed by OpenAI in 2021 ● Rather than rely wholly on image data, queries and keys come from images and text sources respectively ● Compared to Sim-CLR, human created text data can be much more domain/problem specific and can capture much higher fidelity “weakly defined” information than randomised transformations Google Fonts CLIP ● Each font in the google fonts set includes a description provided by the original font’s designer ○ These include things such as … ● For each font, 100 images are produced of random text at different sizes, weights etc. ● The model begins training from OpenAI’s pretrained checkpoint ○ Trained on over 100 million image-text pairs ○ To prevent overfitting, the text encoder’s token embeddings are frozen Martian Mono is a monospaced version of the Martian Grotesk font for code style design. It inherits Grotesk's brutal and eye-catching aesthetics as well as all of its benefits-metrics equilibrium, readability and intelligibility, and convenience for web developers and designers who believe in a systematic approach to design. The McLaren typeface was created to act as a generic go-to comic style lettering. It has simple clean letterforms with a mild bounce and offbeat quality to it without going too far. Vector Search ● Once the model is trained a dataset of vectors can be created, one for each font in the google fonts set ● Given an arbitrary input font, its latent vector can be computed by the model and the dataset can be search to find the most similar fonts ● This process is known as Vector Search and has wide reaching applications in RAG, image search, etc. ● AWS, google, and huggingface all offer hyper-optimised vector search services Examples","libVersion":"0.3.2","langs":""}