{"path":"software engineering/year 2/semester 2/Data Science Fundamentals/Slides/1. Numerical Arrays/1.3 Numerical Arrays, Part 2.pdf","text":"lecture_3_numerical_iii April 4, 2024 1 Lecture 3 Numerical Basics: Part III 1.1 Data Science Fundamentals 1.2 Representation of numbers ##### DSF - University of Glasgow - Chris McCaig - 2021/2022 1.3 Floating point numbers - continued ‚Ä¢ how to compare floating point numbers ‚Ä¢ what machine epsilon is and how it is defined 1.4 Arrays in memory ‚Ä¢ what strided arrays are ‚Ä¢ how striding allows for indexing and arithmetic operations ‚Ä¢ how arrays are laid out in memory ‚Ä¢ uses of higher-rank tensors ‚Ä¢ how to reshape and reorder higher-rank tensors ‚Ä¢ how to use einsum for basic operations [ ]: ##### JUST IN CASE YOU STILL NEED TO INSTALL ANY OF THESE #!pip install -U --no-cache https://github.com/johnhw/jhwutils/zipball/master #!pip install -U scikit-image #!pip install sympy #!pip install statsmodels # try: # import sympy # sympy.init_printing(use_latex='png') # except: # sympy = False # try: # from Tkinter import * 1 # except ImportError: # from tk import * [1]: import IPython.display IPython.display.HTML( \"\"\" <script> function code_toggle() { if (code_shown){ $('div.input').hide('500'); $('#toggleButton').val('Show Code') } else { $('div.input').show('500'); $('#toggleButton').val('Hide Code') } code_shown = !code_shown } $( document ).ready(function(){ code_shown=false; $('div.input').hide() }); </script> <form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\"‚ê£ ‚Ü™value=\"Show Code\"></form>\"\"\" ) [1]: <IPython.core.display.HTML object> [3]: import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt from jhwutils.float_inspector import print_shape_html, print_float,‚ê£ ‚Ü™print_float_html from jhwutils.matrices import show_boxed_tensor_latex, print_matrix #import jhwutils.image_audio as ia import numpy as np from jhwutils.image_audio import ( play_sound, show_image, load_image_colour, load_image_gray, show_image_mpl, load_sound ) 2 %matplotlib inline plt.rc(\"figure\", figsize=(7.0, 3.5), dpi=140) 1.5 Roundoff and precision Floating point operations can introduce roundoff error, because the operations involved have to quantize the results of computations. This can be subtle, because the precision of floating point numbers is variable according to their magnitude; unlike integers, where roundoff is always to the nearest whole number. By Ghennessey - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=35324110 1.5.1 If you take a standard deviation of a collection of values, does it matter if the values are sorted? [94]: np.random.seed(406324) x = np.exp(np.random.uniform(-65, 35, 20000)) x_sorted = np.sort(x) print(\"Unsorted std. dev: %.6f\" % np.std(x)) print(\"Sorted std. dev: %.6f\" % np.std(x_sorted)) ## a terrible order ## alternating largest and smallest x_bad = np.stack([x_sorted[: len(x) // 2], x_sorted[len(x) // 2 :][::-1]]).T. ‚Ü™ravel() print(\"Bad std. dev.: %.6f\" % np.std(x_bad)) Unsorted std. dev: 112247720345711.687500 Sorted std. dev: 112247720345711.656250 Bad std. dev.: 112247720345711.640625 1.5.2 Round off problems [5]: print_float_html(1.0 + 1e-4) # ok <IPython.core.display.HTML object> [95]: print_float_html(1.0 + 1e-8) # ok <IPython.core.display.HTML object> [7]: print_float_html(1.0 + 1e-16) # exactly equal to one <IPython.core.display.HTML object> 3 [96]: print(1e9 + 1e-4) # ok print(1e9 + 1e-8) # exactly equal to 1e9 print(1e9 + 1e-16) # exactly equal to 1e9 1000000000.0001 1000000000.0 1000000000.0 Repeated operations This can be a real problem when doing repeated operations. Imagine adding dividends to a bank account every second (real time banking!) [9]: balance = 100000000000 # Bill Gates' account def accumulate_wealth(starting_balance, dividend_per_second, dtype=np.float64): balance = np.array(starting_balance, dtype=dtype) dividend_per_second = np.array(dividend_per_second, dtype=dtype) seconds_in_month = 24 * 60 * 60 * 30 # run one month's calculation for i in range(seconds_in_month): balance = balance + dividend_per_second print(\"Apparently earned: $%.2f\" % (balance - starting_balance)) print(\"Should have earned: $%.2f\" % (seconds_in_month *‚ê£ ‚Ü™dividend_per_second)) [10]: accumulate_wealth(balance, 0.00001) # got almost twice as much as it should‚ê£ ‚Ü™have! Apparently earned: $39.55 Should have earned: $25.92 [11]: accumulate_wealth(balance, 0.000001) # got nothing at all Apparently earned: $0.00 Should have earned: $2.59 [12]: accumulate_wealth( balance, 0.01, dtype=np.float32 ) # apparently lost money! (original balance wasn't exact in float32) Apparently earned: $-2048.00 Should have earned: $25920.00 Financial operations Moral of the story: Never use floating point operations for financial calculations. Use decimal formats, with precisely defined roundoff rules (if roundoff can occur) for financial operations. 4 1.5.3 Roundoff error While this might seem an unlikely scenario, adding repeated tiny offsets to enormous values is exactly the kind of thing that happens in lots of simulations; for example, plotting the trajectory of a satellite with small forces from solar wind acting upon it. The ordering of operations can be important, with the general rule being to avoid operations of numbers of wildly different magnitudes. This means, in some extreme cases, that the distributive and associative rules don‚Äôt apply to floating point numbers! [13]: (1.0e30 + 1.0) - 1.0e30 # wrong, severe roundoff error [13]: [14]: (1.0e30 - 1.0e30) + 1.0 # no roundoff error [14]: 1.6 Laws of floating-point disaster Some basic rules: 1. x+y will have large error if x and y have different magnitudes (magnitude error) 1. x-y will have large error if x~=y (cancellation error) [15]: print(\"Addition\") print(1.0 + 1.0) # ok print(1.0 + 1e300) # bad, rule 1 Addition 2.0 1e+300 [97]: print(\"Subtraction\") print(1000.0 - 1200.0) # ok print(5000.0 - 5000.0000000000005) # bad, rule 2 Subtraction -200.0 -9.094947017729282e-13 1.6.1 Don‚Äôt compare floats with == Because of the roundoff errors in floating point numbers, whenever there are comparisons to be made between floating point numbers, it is not appropriate to test for precise equality. This code snippet illustrates the problem: 5 [17]: y = 0.1 z = 300000.0 x = y x = x + z x = x - z # x==y, right? print(x == y) print(x - y) False -2.3283069916502086e-11 [18]: # sometimes the error can be quite extreme, # if we have a small value and a large value in one operation x = y + z * z * z x = x - z * z * z print(x - y) print(x) ## uh oh, x has become 0! -0.1 0.0 NEVER USE EQUALITY ON FLOATING POINT VALUES OR ARRAYS! (unless you are explicitly testing for roundoff error and know what you are doing) Instead, we must always compare floating point numbers by determining if their absolute difference is less than some threshold: |ùë• ‚àí ùë¶| < ùúñ [99]: y = 0.1 z = 300000.0 x = y x = x + z x = x - z eps = 1e-8 # this threshold is often called epsilon # but is not the same as machine epsilon! # (1e-8 is not universal, but often reasonable for float64) # in practice, the choice of epsilon will depend on the application and the‚ê£ ‚Ü™operations performed print(abs(x - y) < 1e-8) True 6 However, if results might span many orders of magnitude, a relative test may be more appropriate. There is no one perfect choice. [20]: print(abs(x / y) - 1.0 < 1e-8) # relative tolerance True 1.6.2 Allclose Testing if all elements of two arrays are within some tolerance is a standard array operation, which NumPy calls np.allclose(x,y). This combines absolute and relative tests in a sensible way. [100]: print(np.allclose(x, y)) True [22]: x = np.full((8, 8), 33341331514515.110) y = np.full((8, 8), 7452.15415) z = (x * y * y * y) / (y * y * y) print(x == z) [[False False False False False False False False] [False False False False False False False False] [False False False False False False False False] [False False False False False False False False] [False False False False False False False False] [False False False False False False False False] [False False False False False False False False] [False False False False False False False False]] [23]: print(np.all(x == z)) False [24]: print(np.allclose(x, z)) True 1.6.3 Machine precision and ùúñ Just how bad (or good) are floating point numbers at representing real numbers within their range? How close is float(ùúã) to ùúã? Floating point representations have a relative error, defined by: ùúñ = |float(ùë•) ‚àí ùë•| |ùë•| , 7 the absolute difference between a floating point number and its real counterpart, normalised by the magnitude of the real number. IEEE 754 guarantees that this error is always less than ùúñ ‚â§ 1 2 2‚àíùë°, ùúñ ‚â§ 2‚àíùë°‚àí1, where ùë° is the number of bits dedicated to the mantissa, excluding the implied 1. This guarantee applies to both storage of numbers (the relative error will never be greater than ùúñ) and to operations on numbers (that roundoff error in computations will be have relative error < ùúñ). For float64, ùúñ = 2‚àí53 ‚âà 1.1 √ó 1016. [25]: # we can ask NumPy to print these statistics for us # note the epsilon term epsneg print(np.finfo(np.float64)) Machine parameters for float64 --------------------------------------------------------------- precision = 15 resolution = 1.0000000000000001e-15 machep = -52 eps = 2.2204460492503131e-16 negep = -53 epsneg = 1.1102230246251565e-16 minexp = -1022 tiny = 2.2250738585072014e-308 maxexp = 1024 max = 1.7976931348623157e+308 nexp = 11 min = -max smallest_normal = 2.2250738585072014e-308 smallest_subnormal = 4.9406564584124654e-324 --------------------------------------------------------------- [26]: # check this is indeed the value we expect to see t = 52 2.0 ** (-t - 1) [26]: 2 Array layout and structure 3 ndarrays A 256x128x4 array of floating point numbers takes up 1MB of memory. We can check this: [27]: array = np.zeros((256, 128, 4)) print(array.nbytes) 8 1048576 (this count ignores any header information, which will be a small, nearly constant overhead for any array) What‚Äôs actually in that 1MB of memory? How are operations like: array.T[:,::2] += 1 implemented? There are two parts to this problem: * how arrays are represented * how numbers are represented (which Part I covered) 3.1 How arrays are laid out in memory Multidimensional numerical arrays (ndarrays for short) are an eÔ¨Äicient data structure, in both memory and computational terms. If we recall the key properties of ndarrays from Lecture I: ‚Ä¢ fixed, predefined size (or ‚Äúshape‚Äù) ‚Ä¢ rectangular ‚Ä¢ fixed uniform type (all elements have the same type) ‚Ä¢ elements are numbers ‚Ä¢ arrays are multidimensional It is important to understand how array datastructures are implemented to understand the impact of operations: ‚Ä¢ why is transpose O(1) time, but adding 1 to an array O(N) time? ‚Ä¢ Why does equality testing on arrays not do what you expect? ‚Ä¢ What is the result np.ones((3,3)) / np.zeros((3,3))? ‚Ä¢ Why are ragged arrays not supported? 3.2 Array data structure Specific implementations differ on exactly how they implement arrays. But a common feature is that the data in the arrays is tightly packed in memory, such that the memory used by an array is a small, constant overhead over the storage of the numbers that make up its elements. There is a short header which describes how the array is laid out in memory, followed by the numerical data. EÔ¨Äiciency is achieved by packing in the numbers in one long, flat sequence, one after the other. Regardless of whether there is a 1D vector or a 5D tensor, the storage is just a sequence of numbers with a header at the start. We can see this flat sequence using np.ravel(), which ‚Äúunravels‚Äù an array into the elements as a 1D vector; in reality it is just returning the array as it is stored in memory. (caveat: this isn‚Äôt quite the order in memory by default; we‚Äôll deal with this later). [101]: one_d = np.array([1.0, 2.0, 3.0]) two_d = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], 9 [7.0, 8.0, 9.0]]) show_boxed_tensor_latex(one_d) show_boxed_tensor_latex(two_d) 1 2 3 1 2 3 4 5 6 7 8 9 [29]: four_d = np.tile(two_d, (3, 2, 1, 1)) show_boxed_tensor_latex(four_d) 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 [30]: print(one_d.ravel()) # obvious enough [1. 2. 3.] [31]: print(two_d.ravel()) # rows, then columns [1. 2. 3. 4. 5. 6. 7. 8. 9.] [32]: print(four_d.ravel()) show_boxed_tensor_latex(four_d) [1. 2. 3. 4. 5. 6. 7. 8. 9. 1. 2. 3. 4. 5. 6. 7. 8. 9. 1. 2. 3. 4. 5. 6. 7. 8. 9. 1. 2. 3. 4. 5. 6. 7. 8. 9. 1. 2. 3. 4. 5. 6. 7. 8. 9. 1. 2. 3. 4. 5. 6. 7. 8. 9.] 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 3.2.1 Strides and shape To implement multidimensional indexing, a key property of arrays, the standard ‚Äútrick‚Äù is to use striding, with a set of memory offset constants (‚Äústrides‚Äù) which specify how to index into the array, one per axis. This lets the system eÔ¨Äiciently index into the array as if it were multidimensional, while keeping it as a single long sequence of numbers packed tightly into memory. 10 There is one stride per dimension, and each stride tells the system how many bytes forward to seek to find the next element for each dimension (it‚Äôs called a ‚Äústride‚Äù because it‚Äôs how big of a step in memory to take to get to the next element). For a 1D array, there is one stride, which will be the length of the numeric data type (e.g. 8 bytes for a standard float64). For a 2D array x, the first element might be 8 (one float), and the second might be 8*x.shape[0]. In other words, to move to the next column, add 8; to move to the next row, add 8 times the number of columns. Strides are usually given in bytes, not elements, to speed up memory access computations. A strided 2D array To find the array element at index [i,j] in a 2D matrix, the memory offset from the start of the number block will be: i * stride[0] + j * stride[1] This generalises to higher dimensions (e.g. 3D, 4D tensors). To iterate through an array, the computations can simply increment by the appropriate stride to move to the next elements; for most operations, though, it is suÔ¨Äicient to keep incrementing by the first stride to visit each element in turn. 3.2.2 Dope fiends This type of representation is correctly called a dope vector, where the dope vector refers to the striding information. It is held separately from the data itself; a header which specifies how to index. The alternative is an lliffe vector, which uses nested pointers to refer to multidimensional arrays. This is what happens, for example, if we make a list of lists in Python: [[1,2,3], [4,5,6], [7,8,8]] The outer list refers to three inner lists. lliffe vectors can store ragged arrays trivially (no rectangularity requirement), but are much less eÔ¨Äicient for large numerical operations that a form using dope vectors. Java Illife vector int[][] a = new int[8][8]; elt_3_4 = a[3][4] Java dope vector int [] a = new a[64]; row_offset = 8; elt_3_4 = a[row_offset*3 + 4]; 3.2.3 A sketch of an array data structure A typical structure might look like this (some details omitted). This is written as a C struct, but you can imagine equally well as a Java or Python class. This structure assumes the dtype is fixed to float64 (double in C). 11 // assume uint is a suitable unsigned integer type, // e.g. uint64_t // assume double is IEEE754 float64 struct NDArray // assumes numbers are all doubles { uint n_dim; // number of dimensions uint n_items; // total number of elements uint item_size; // size of one element (in bytes) uint *shape; // shape, sequence of unsigned ints int *strides; // striding, as a sequence of integers, // in *bytes* (may be negative!) double *data; // pointer to the data array, // in this case float64 (double) uint flags; // any special flags }; Most of these are self explanatory; e.g. for a 3x3 array of float64 holding all zeros we would have: n_dim = 2 // 2D, i.e. a matrix n_items = 9 // 3 * 3 = 9 elements item_size = 8 // 8 bytes per float64 shape = {3,3} // this is just the shape of the array strides = {8, 24} // the offsets, in bytes, // to move to the next element in each dimension data = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0} // we'll ignore flags 3.2.4 Vectorised operations Note that many operations can now be defined that just ignore the strides. For example, adding together two arrays: // some pseudo code if(same_shape(x,y)) { z = zero_array_like(x); for(i=0;i<x->n_items;i++) z->data[i] = x->data[i] + y->data[i]; return z; } 3.2.5 Strided arrays in practice This is pretty much how NumPy arrays are implemented, and we can query such properties directly on NumPy arrays: 12 [33]: mat = np.arange(30.0).reshape((6, 5)) print_shape_html(mat) <IPython.core.display.HTML object> [34]: show_boxed_tensor_latex(mat) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 3.2.6 Transposing This lets us do some clever things. For example, to transpose an array, all we need to do is to exchange the shape elements and the strides in the header. The elements themselves are not touched, and the operation takes constant time regardless of array size. Operations simply read the memory in a different way after the transpose. Or we can reverse the array by setting the memory block pointer to point to the end of the array, and then setting the strides to be negative. 3.2.7 Rigid transformations Rigid transformations of arrays like flipping and transposing do not change the data in the array. They only change the strides used to compute indexing operations. This means they do not depend on the size of the array, and are thus O(1) operations. (technically they are O(D) where D is the number of dimensions of the array) [35]: # now transpose print(\"Transposed\") mat_t = mat.T print_shape_html(mat_t) Transposed <IPython.core.display.HTML object> [36]: show_boxed_tensor_latex(mat.T) 0 5 10 15 20 25 1 6 11 16 21 26 2 7 12 17 22 27 3 8 13 18 23 28 4 9 14 19 24 29 [37]: # flip flipud = np.flipud(mat) print(\"Flipped up/down\") print_shape_html(flipud) 13 show_boxed_tensor_latex(flipud) Flipped up/down <IPython.core.display.HTML object> 25 26 27 28 29 20 21 22 23 24 15 16 17 18 19 10 11 12 13 14 5 6 7 8 9 0 1 2 3 4 [38]: # flip print(\"Flipped left/right\") fliplr = np.fliplr(mat) print_shape_html(fliplr) show_boxed_tensor_latex(fliplr) Flipped left/right <IPython.core.display.HTML object> 4 3 2 1 0 9 8 7 6 5 14 13 12 11 10 19 18 17 16 15 24 23 22 21 20 29 28 27 26 25 [39]: # rot90 rotated = np.rot90(mat) print(\"rotated 90\") print_shape_html(rotated) show_boxed_tensor_latex(rotated) rotated 90 <IPython.core.display.HTML object> 4 9 14 19 24 29 3 8 13 18 23 28 2 7 12 17 22 27 1 6 11 16 21 26 0 5 10 15 20 25 3.2.8 Transpose has time complexity O(1) [102]: N = 100 x = np.zeros((N, N)) [103]: %%timeit -n 100 x + 1 # elementwise operation: O(N) The slowest run took 6.15 times longer than the fastest. This could mean that an intermediate result is being cached. 4.52 ¬µs ¬± 4.38 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each) 14 [104]: %%timeit -n 100 x.T # stride operation O(1) 105 ns ¬± 99.8 ns per loop (mean ¬± std. dev. of 7 runs, 100 loops each) 3.2.9 C and Fortran order There is one thing you might have noticed: NumPy gave the strides of the original array as [48,8], not [8,48]. This is because the NumPy default (and most common order generally) is that the last index changes first. This applies to any higher dimensional array as well: for example a (512, 512, 3) RGB color image in C order has the memory layout R -> G -> B -> R G B R G B -> going column-wise -> then row-wise This ordering is called C ordering or row-major, and is the default in C-based languages. The alternative, where the first index changes fastest is called Fortran ordering or column-major and is sometimes still used in older software. In other words, iterating over an array in these orders would work like the following pseudo-code. The elements are stored one after the other; it is just the convention by which we do the indexing computation. #### C ordered filling of an 4D tensor [m,n,o,p] index = 0 for mi in range(m): for ni in range(n): for oi in range(o): for pi in range(p): tensor_memory[index] = 0.0 index += 1 Fortran ordered filling of an 4D tensor [m,n,o,p] index = 0 for pi in range(p): for oi in range(o): for ni in range(n): for mi in range(m): tensor_memory[index] = 0.0 index += 1 [43]: x = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[0, 0, 0], [0, 0, 0], [0, 0,‚ê£ ‚Ü™0]]]) show_boxed_tensor_latex(x) 1 2 3 4 5 6 7 8 9 0 0 0 0 0 0 0 0 0 [44]: x.ravel(order=\"C\") 15 [44]: array([1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]) [45]: x.ravel(order=\"F\") [45]: array([1, 0, 4, 0, 7, 0, 2, 0, 5, 0, 8, 0, 3, 0, 6, 0, 9, 0]) 3.3 NumPy types Unless you specify a different type (e.g. with dtype= when calling np.array or np.zeros, etc.), arrays will be float64 if they have any floating point value when created, and int32 otherwise. An array can be converted using .astype(), which will convert the array elements to the specified type. [46]: x = np.array([1, 2, 3]) # no floats, this will be int32 print(x.dtype) int64 [47]: x = np.array([1, 2, 3], dtype=np.int8) # force to int8 print(x.dtype) int8 [48]: y = np.array([1.0, 2, 3]) # at least one float, will be float64 print(y.dtype) float64 [49]: x_float = x.astype(np.float64) print(x_float.dtype) print(x_float) float64 [1. 2. 3.] [50]: x_float32 = x.astype(np.float32) print(x_float32.dtype) print(x_float32) float32 [1. 2. 3.] 4 Example: Brainwaves .Image by Tim Sheerman-Chase license CC BY 16 I have a recording of brain activity from an electroencephalogram (EEG). This consists of a series of electrical measurements, over 16 electrodes arranged on the scalp. Measurements of voltage on the scalp are captured regularly in time at a fixed rate, 512Hz (512 times a second). In its basic form, the data might be a ùëÅ √ó ùê∑ matrix for ùëÅ measurements over time, for each of the ùê∑ electrodes. One second of data would be a 512 √ó 16 matrix in this configuration Brain activity is very diÔ¨Äicult to interpret from these signals. However, gross changes in activity can be seen in the frequencies of oscillation in each electrode (e.g. ‚Äúalpha wave‚Äù activity). The data can be transformed, so that each electrode can be mapped to a number of frequencies each of which has some level of activation. This mapping might generate 64 frequency bands for each electrode. Now 1 second of data might be represented as a 16 √ó 64 √ó 32 array, for one second of data. [51]: eeg_data = np.load(\"data/jhw_eeg.dat.npz\") time_series = eeg_data[\"raw_data\"] brain_data = eeg_data[\"brain_data\"] t = eeg_data[\"brain_t\"] f = eeg_data[\"brain_freq\"] [52]: fig = plt.figure() ax = fig.add_subplot(1, 1, 1) for i in range(1, 18): ax.plot(time_series[:, 0] / 1e3, time_series[:, i] + i * 200, color=\"k\",‚ê£ ‚Ü™lw=0.5) ax.set_title(\"16 channel EEG time series\") ax.set_xlabel(\"Time (seconds)\") [52]: Text(0.5, 0, 'Time (seconds)') 17 [53]: # 16 channels; 64 frequencies; 166 time steps\\ print(brain_data.shape) (16, 64, 166) [54]: # show a slice of activity from one channel (one electrode) plt.rc(\"figure\", figsize=(5.0, 2.5), dpi=140) fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.imshow((brain_data[5, :, :]), origin=\"upper\", aspect=\"auto\", extent=(t[0],‚ê£ ‚Ü™t[-1], f[0], f[-1]), vmin=0, vmax=505,) plt.title(\"Channel 11\") ax.set_xlabel(\"Time (seconds)\") ax.set_ylabel(\"Frequency (Hz)\") ax.set_xlim(4, 154) ax.set_ylim(0.5, 20) [54]: Now imagine I want to build a machine learning system to classify different kinds of brain signals. This would involve chopping up the signal into small chunks (‚Äúfeatures‚Äù), and then assembling these chunks into batches. One batch might be 10 blocks of 15 timesteps of data; this is a 16 √ó 64 √ó 15 √ó 10 array ‚Äì a rank 4 tensor. 18 [55]: # chop the brain data into 15 timestep chunks, stacked up in an array # channels x frequencies x timesteps x chunks chunk = 15 chunked = brain_data[:, :, :150].reshape(16, 64, chunk, -1) print(chunked.shape) (16, 64, 15, 10) Now I could reduce; for example take the median across the time slices and across the electrodes. [56]: # reduction operation plt.rc(\"figure\", figsize=(7.0, 3.5), dpi=140) mean_activation = np.median(chunked, axis=(0, -1)) # compute median across‚ê£ ‚Ü™electrodes and chunks plt.imshow(mean_activation, origin=\"upper\") [56]: <matplotlib.image.AxesImage at 0x7f7c9da3d090> 5 Tensor operations 1D and 2D matrices are reasonably straightforward to work with. But many interesting data science problems like the EEG example involve arrays with higher rank; this is very common in deep learning, for example. Being able to think about vectorised operations and then mold tensors to the right configurations to do what you want eÔ¨Äiciently is a specialised and valuable skill. 19 5.1 Reshape As well as ‚Äúrigid‚Äù transformations like np.fliplr() and np.transpose(), we can also reshape arrays to completely different shapes. The requirement is that the elements don‚Äôt change; thus the total number of elements cannot change during a reshaping operation. Regardless of how many dimensions an array has, it is still inherently a sequence of values; the shape just changes how it is indexed. In other words, it just writes a new set of strides. ravel() that we saw earlier, will show the ‚Äúflat‚Äù representation: [57]: x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) show_boxed_tensor_latex(x) print(x.ravel()) # unsurprising 1 2 3 4 5 6 7 8 9 10 11 12 [ 1 2 3 4 5 6 7 8 9 10 11 12] [58]: y = x.reshape((3, 4)) show_boxed_tensor_latex(y) print( y.ravel() ) # note: order is columns, then rows; always working from the last dimension‚ê£ ‚Ü™back to the first 1 2 3 4 5 6 7 8 9 10 11 12 [ 1 2 3 4 5 6 7 8 9 10 11 12] 5.1.1 Reshaping rules ‚Ä¢ The total number of elements will remain unchanged. ‚Ä¢ The order of elements will remain unchanged; only the positions at which the array ‚Äúwraps‚Äù into the next dimension. ‚Ä¢ The last dimension changes fastest, the second last second fastest, etc. The pouring rule You can imagine a reshaping operation ‚Äúpouring‚Äù the elements into a new mold. The ordering of the elements is retained, but will fill up the new shape. Pouring fills up the last dimension first. [105]: x = np.arange(12) # 12 elt vector x3x4 = x.reshape((3, 4)) show_boxed_tensor_latex(x) show_boxed_tensor_latex(x3x4) 20 0 1 2 3 4 5 6 7 8 9 10 11 0 1 2 3 4 5 6 7 8 9 10 11 [60]: show_boxed_tensor_latex(x.reshape((6, 2))) 0 1 2 3 4 5 6 7 8 9 10 11 [61]: show_boxed_tensor_latex(x.reshape((4, 3))) 0 1 2 3 4 5 6 7 8 9 10 11 [62]: show_boxed_tensor_latex(x.reshape((2, 3, 2))) # last dimension, then columns,‚ê£ ‚Ü™then rows 0 1 2 3 4 5 6 7 8 9 10 11 [63]: show_boxed_tensor_latex(x.reshape((4, 3, 1))) # Can have 1 in any dimension 0 1 2 3 4 5 6 7 8 9 10 11 [64]: show_boxed_tensor_latex(x.reshape((1, 3, 4))) # A different shape 0 1 2 3 4 5 6 7 8 9 10 11 5.1.2 Advanced reshaping: squeezing and adding dimensions Imagine we want to multiply two vectors together (we‚Äôll see how this works in detail later). There are two possible orientations for a vector: a row vector and a column vector. Multiplying a row vector by a column vector results in a matrix (the outer product): 21 [65]: x = np.array([1, 2, 3]) y = np.array([-1, 9, 1]) print(np.dot(x, y)) # scalar product, row by row 20 [66]: print(np.dot(x, y.T)) # OK, so matrix product -- right? 20 [67]: print(y) print(y.T) # huh? this does nothing? [-1 9 1] [-1 9 1] Transposing a 1D vector has no effect ‚Äì it has only one dimensions and reversing the shape does nothing. So how can we get the effect of a column vector? In fact, a 1D vector is neither a row vector or a column vector. It is just a 1D vector. The operations we might expect are operations on 2D matrices with a singleton dimension. This requires promoting the vector to a matrix, by adding a new singleton dimension. In less fancy terms, it which just means adding a new dimension with a shape of 1: [68]: y_col = np.reshape(y, (3, 1)) # 3 rows x 1 column x_col = np.reshape(x, (3, 1)) print_matrix(\"x\\cdot y^T\", np.dot(x_col, y_col.T)) ùë• ‚ãÖ ùë¶ùëá = ‚é° ‚é¢ ‚é£ ‚àí1 9 1 ‚àí2 18 2 ‚àí3 27 3 ‚é§ ‚é• ‚é¶ 5.1.3 Adding dimensions Because adding singleton dimensions is so common, there is special syntax for it. You can index an array with an additional dimension to promote it to a higher rank, if the index is given as None or np.newaxis (they have the same effect). This inserts a new singleton dimension into the strides. [69]: x = np.array([1, 2, 3]) # back to 1D [70]: print(x[:].shape) # just x show_boxed_tensor_latex(x) (3,) 1 2 3 [71]: print(x[None, :].shape) # add a new dimension 22 (1, 3) [72]: print(x[:, np.newaxis].shape) # add a new dimension (identical to the above) show_boxed_tensor_latex(x[:, np.newaxis]) (3, 1) 1 2 3 [73]: print(x[None, None, :, None].shape) # add lots of new dimensions show_boxed_tensor_latex(x[None, None, :, None]) (1, 1, 3, 1) 1 2 3 What if we want to remove dimensions? We can simply index to do this, as indexing will remove a dimension from a tensor. If there is only element, this just removes the singleton dimension. [74]: x_4d = x[None, None, :, None] show_boxed_tensor_latex(x_4d) show_boxed_tensor_latex(x_4d[0, 0, :, 0]) 1 2 3 1 2 3 5.1.4 Squeezing Singleton dimensions can get in the way of doing computations easily; often the result of a complex calculation is a multidimensional array with a bunch of singleton dimensions. squeezing just removes all singleton dimensions in one go and is performed by np.squeeze() [75]: show_boxed_tensor_latex(x_4d) show_boxed_tensor_latex(np.squeeze(x_4d)) # back to 3 element vector 1 2 3 23 1 2 3 [76]: # 3x4 array mat3x4 = np.arange(12).reshape((3, 4)) show_boxed_tensor_latex(mat3x4) 0 1 2 3 4 5 6 7 8 9 10 11 [77]: # insert some new dimensions (note that we can insert dimensions in between‚ê£ ‚Ü™existing ones!) mat3x4_5d = mat3x4[:, None, None, :, None] print(mat3x4_5d.shape) show_boxed_tensor_latex(mat3x4_5d) (3, 1, 1, 4, 1) 0 1 2 3 4 5 6 7 8 9 10 11 [78]: show_boxed_tensor_latex(np.squeeze(mat3x4_5d)) 0 1 2 3 4 5 6 7 8 9 10 11 5.1.5 Elided axes With large tensors it can be annoying to specify all the intermediate dimensions in an indexing operation, with lots of code that looks like: img_tensor[0,:,:,:,4] Repeated : can be elided in an indexing expression using an ellipsis (three dots) img_tensor[0, ..., 4] the dots replace as many : as required to make the shape of the index match the shape of the array. [79]: mat3x4_5d[0, :, :, :, 0] [79]: array([[[0, 1, 2, 3]]]) 24 [80]: mat3x4_5d[0, ..., 0] [80]: array([[[0, 1, 2, 3]]]) [81]: mat3x4_5d[0, :, :, 3, 0] [81]: array([[3]]) [82]: mat3x4_5d[0, ..., 3, 0] # ... fills as many dimensions as required [82]: array([[3]]) 5.2 Swapping and rearranging axes We can rearrange the axes of our arrays as we wish, using np.swapaxes(a, axis1, axis2) to swap any pair of axes. For example, if we have a colour video, which is of shape (frames, width, height, 3) and we want to apply an operation on each column, we can temporarily swap it to the end, broadcast, then swap it back. This is the generalisation of the transpose trick we saw earlier: [83]: x = np.array([[0,0,0], [0,0,1]]) y = np.array([1,2]) (x.T + y).T [83]: array([[1, 1, 1], [2, 2, 3]]) (x.T + y).T # adds y to every column of x This again is just an operation which rewrites the strides. [84]: cat = load_image_colour(\"imgs/Belle.jpg\") show_image(cat, width=\"300px\") print(cat.shape) 25 (1333, 1000, 3) 26 [85]: # swap the columns to the end; multiply by a gradient, broadcasting; swap back gradient_cat = (np.swapaxes(cat, 0, 2) * np.linspace(0, 1, cat.shape[0])).swapaxes(2, 0) show_image(gradient_cat, width=\"300px\") 27 Axis rearrangement is (usually) a simple change of the array strides and shape; the array itself is not changed, and so the operations completes in ùëÇ(1) time. [86]: print(cat.strides) (24000, 24, 8) [87]: # note that all we did was permute the strides in the array # this is super fast :) print(cat.swapaxes(1, 2).strides) (24000, 8, 24) 5.2.1 The swap, reshape, swap dance Reshape always follows the pouring rule (last dimension pours first). Sometimes that isn‚Äôt what we want to do. The solution is to: * rearrange the axes * reshape the array * (optionally) rearrange the axes again Imagine we want to get the cat gif as a film strip, splitting the colour channels into three rows, one for red, green and blue, as if we had three strips of celluloid film. This will be a reshape to size (H*3, W*Frames). [88]: cat_strip = cat.reshape(cat.shape[1] , cat.shape[2] * cat.shape[0]) show_image(cat_strip) # this not be good [89]: cat_strip = ( cat.swapaxes(2, 0) .swapaxes(2, 1) .reshape(cat.shape[1] , cat.shape[2] * cat.shape[0]) ) show_image(cat_strip) # this is correct show_image(cat_strip[:, :2000]) # zoomed in version 28 5.2.2 Einstein summation notation Image credit: public domain, from Wikimedia Commons A very powerful generalisation of these operations is Einstein summation notation. This, in its simplest form, is a very easy way to reorder higher-rank tensors. As you can see from the example above, swapping axes gets confusing very fast. Einstein summation notation allows a specification of one letter names for dimensions (usually from ijklmn...), and then to write the dimension rearrangement as a string. The notation writes the original order, followed by an -> arrow, then a new order, like this: ijk -> jik For example, 2D matrix transpose can be written like this: [90]: x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) show_boxed_tensor_latex(x) print() 29 show_boxed_tensor_latex(x.T) print() 1 2 3 4 5 6 7 8 9 1 4 7 2 5 8 3 6 9 [91]: # switch first and second dimensions print(\"Transposed via einsum\\n\") show_boxed_tensor_latex(np.einsum(\"ij -> ji\", x)) Transposed via einsum 1 4 7 2 5 8 3 6 9 [92]: # einsum is really useful for higher order arrays y = np.zeros((5, 4, 10, 10)) y_rearranged = np.einsum(\"ijkl -> jlik\", y) print(\"Y shape\", y.shape) print(\"Y strides\", y.strides) print() print(\"Y rearranged\", y_rearranged.shape) print(\"Y rearranged strides\", y_rearranged.strides) Y shape (5, 4, 10, 10) Y strides (3200, 800, 80, 8) Y rearranged (4, 10, 5, 10) Y rearranged strides (800, 8, 3200, 80) [93]: # slightly easier to understand # we switch from # (f)rames, (r)ows, (c)olumns to # (r)ows, (f)rames, (c)columns cat_strip = np.einsum(\"frc->crf\", cat).reshape( cat.shape[1] , cat.shape[2] * cat.shape[0] ) show_image(cat_strip) 30 5.2.3 Extreme einsumming einsum can also compute summations, products and diagonalisations in one single command, but this is beyond what we will cover in DSF. The power of einsum leverages the flexibility of the ndarray strided array structure. It makes a huge range of operations possible in a very compact form. Optional reading Read A guide to Einstein sums http://ajcr.net/Basic-guide-to-einsum/ if you want more details on using np.einsum. 6 Resources for this lecture ‚Ä¢ From Python to Numpy http://www.labri.fr/perso/nrougier/from-python-to-numpy/ rec- ommended reading ‚Ä¢ Floating point numbers http://pmihaylov.com/floating-point-numbers/ (floats) ‚Ä¢ What Every Computer Scientist Should Know About Floating Point Numbers http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html 6.1 Advanced (beyond this course) ‚Ä¢ Advanced NumPy http://www.scipy-lectures.org/advanced/advanced_numpy/ ‚Ä¢ NumPy tricks http://arogozhnikov.github.io/2015/09/29/NumpyTipsAndTricks1.html and http://arogozhnikov.github.io/2015/09/30/NumpyTipsAndTricks2.html [ ]: 31","libVersion":"0.3.2","langs":""}