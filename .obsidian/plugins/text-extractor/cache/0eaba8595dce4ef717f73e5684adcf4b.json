{"path":"software engineering/year 2/semester 1/Practical Algorithms/Slides/2. Algorithm Analysis/Algorithm Analysis.pdf","text":"Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size â†’ ð‘‡(ð‘›) 3. Finding the dominant part of that function that represents its growth rate or â€œbig-Ohâ€ complexity Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size â†’ ð‘‡(ð‘›) 3. Finding the dominant part of that function that represents its growth rate or â€œbig-Ohâ€ complexity First: A Model of Implementation Hardware â€¢ We assume a generic processor, where instructions are executed one after another, with no concurrent operations. â€¢ This is called the RAM model â€¢ In real-life situations involving processors with parallel processing capabilities (which is most processors now), concurrency considerations need to be taken into account â€¢ We arenâ€™t going there in this course, but go here for a quick deep dive This Photo by Unknown Author is licensed under CC BY-SA Identifying Primitive operations (â€œstepsâ€) in Algorithmic Analysis â€¢ Basic computations performed by an algorithm â€¢ Low-level instructions commonly found in real computers: arithmetic (+, -, /, x), data movement, control. â€¢ Assumed to take a common amount of time â€¢ Identifiable in pseudocode â€¢ Largely independent from the programming language â€¢ We ignore memory hierarchies, cache hits/misses etc â€¢ Exact definition not important (We will see why later) â€¢ Examples â€¢ Fundamental arithmetic operations (ie: addition, multiplication, â€¦) â€¢ Value assignment to a variable â€¢ Array indexing â€¢ Function call â€¢ (only the act of calling a function is â€œprimitiveâ€; executing that called function can span many primitive operations, and will need to be analysed in its own right) â€¢ Returning from a method Which of these is a primitive operation? â€¢ val = a+b â€¢ array2 = sort_ascending(array1) Which of these is a primitive operation? â€¢ val = a+b YES â€¢ array2 = sort_ascending(array1) NO! Counting primitive operations â€¢ By inspecting the pseudocode, we can determine: â€¢ the worst-case (ie, maximum) number of primitive operations executed by an algorithm, â€¢ as a function of the input size ð¼ð‘›ð‘ð‘¢ð‘¡ð‘†ð‘–ð‘§ð‘’ Õœ ð‘“ ð‘€ð‘Žð‘¥ð‘‚ð‘s or ð‘› Õœ ð‘“ ð‘‡(ð‘›) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max â€¢ Given an Array A of numerical values, find its maximum element: Counting Primitive Operations Example: â€œArrayMaxâ€ Problem Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 assignment and array indexing 55 Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n assignment and test In general, the loop header is executed one time more than the loop body 56 Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) array indexing and test 57 Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) array indexing and assignment 58 Note: Worst case analysis â€“ we assume max is updated at every iteration Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) assignment and addition 59 Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) 1 return 60 Counting primitive operations â€¢ By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an algorithm, as a function of the input size â€¢ Example: find the maximum value in an array A of integers (with indices between 0 and n-1) ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max Operations 2 1+n 2(n-1) 2(n-1) 2(n-1) 1 Total 7n - 2 61 Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size â†’ ð‘‡(ð‘›) 3. Finding the dominant part of that function that represents its growth rate or â€œbig-Ohâ€ complexity Estimating running time from counted operations Algorithm ARRAY-MAX executes 7n âˆ’ 2 primitive operations in the worst case â€¢ Worst case being: input array is in ascending order with maximum value at last position; hence max updated on every step As discussed earlier, we assume all steps take the same amount of time*, so the time taken is is directly proportional to the number of steps *Armed with your know-how of Assembly language, you know better! E.g. a = b+c and a = (b+c)/(d+e), both may look like a single step in a high-level language, but the latter will translate to more machine level instructions or â€œstepsâ€. For such analysis as we are doing now though, these variations can be ignored. Estimating running time from counted operations Algorithm ARRAY-MAX executes 7n âˆ’ 2 primitive operations in the worst case â€¢ Worst case being: input array is in ascending order with maximum value at last position; hence max updated on every step As discussed earlier, we assume all steps take the same amount of time*, so the time taken is is directly proportional to the number of steps So: ð‘‡ ð‘› = 7ð‘› âˆ’ 2 *Armed with your know-how of Assembly language, you know better! E.g. a = b+c and a = (b+c)/(d+e), both may look like a single step in a high-level language, but the latter will translate to more machine level instructions or â€œstepsâ€. For such analysis as we are doing now though, these variations can be ignored. It gets simpler! The actual asymptotic analysis can be done way more simply than what we just did. We donâ€™t need an exact expression for number steps. We only need an estimate of how quickly the function grows as the problem size increases. That is, we can ignore lower-order terms Or, in other words, we are only interested in the big-Oh complexity of ð‘‡(ð‘›) 65 This Photo by Unknown Author is licensed under CC BY-SA Determining big-O complexity: 1. Counting primitive operations 2. Expressing them as a function of the problem size â†’ ð‘‡(ð‘›) 3. Finding the dominant part of that function that represents its growth rate or â€œbig-Ohâ€ complexity Thinking about the growth rate of running time â€¢ Expressions like 7n-2, or 6x2+2x+124, are â€œtoo preciseâ€ for the purposes of asymptotic analysis â€¢ Context: Analyse the complexity of an algorithm by estimating how the number of steps grow, in the limit as the size of the problem grows Õœ âˆž â€¢ For an algorithm like ARRAY-MAX: â€¢ It is the linear growth rate of the running time T(n) the intrinsic property of the algorithm that we wish to focus on â€¢ For example, the following running times are all (asymptotically) linear: 7ð‘› âˆ’ 2 = ð‘‚(ð‘›) 7239ð‘› = ð‘‚(ð‘›) 9 350 ð‘› + 100 = ð‘‚(ð‘›) Managing (simplifying) asymptotic analysis â€¢ We can simplify the analysis by looking at the behaviour of different factors that we may find in a ð‘‡(ð‘›) expression: â€¢ Constant factors â€¢ Linear factors â€¢ Power/Exponential/Factorial factors The Insight â€¢ Growth rate is not affected by â€¢ constant factors â€¢ lower-degree terms â€¢ That is, at large enough n, the function ð‘‡(ð‘›) is largely determined by the highest degree term in its expression â€¢ So: Big-Oh notation is used to express asymptotic upper bounds This Photo by Unknown Author is licensed under CC BY-SA Ignore constant factors â€¢ Example A = 102n + 105 is a linear function A = B + C, where B = 102n is a linear function C = 105 is a constant (impact on growth rate can be ignored) Ignore lower-degree terms â€¢ Example D = 105n2 + 108n is a quadratic function E = 105n2 is a quadratic function F = 108n is a lower-degree (linear) term (impact on growth rate can be ignored) D = E + F Key Intuition (informal) Notation ð‘“ ð‘¥ = ð‘‚ ð‘”(ð‘¥ ) Is the asymptotic analogue of traditional ordering relation ð‘“ ð‘¥ â‰¤ ð‘”(ð‘¥) That is, ð‘”(ð‘¥) provides an asymptotic upper-bound to ð‘“(ð‘¥) Rules for finding the Big-O complexity of algorithms Rules to compute running times â€¢ Rule 1 â€“ Loops âˆ’ The running time of a loop is at most the running time of the statements inside the loop (including loop bound tests) multiplied by the number of iterations ALG1(n) for i = 0 to n-1 increment x Rules to compute running times â€¢ Rule 1 â€“ Loops âˆ’ The running time of a loop is at most the running time of the statements inside the loop (including loop bound tests) ) multiplied by the number of iterations â€¢ Rule 2 â€“ Nested loops âˆ’ Total running time of a statement inside a group of nested loops is running time of statement multiplied by the product of the sizes of all the loops âˆ’ This way we focus on the fastest changing (inner-most) loop, as that will determine the dominant term ALG1(n) for i = 0 to n-1 for j = 0 to n-1 for k = 0 to n-1 increment x â€¢ Rule 3 â€“ Consecutive statements âˆ’ Just add Rules to compute running times ALG1(n) for i = 0 to n-1 read x from array increment x store x back in array â€¢ Rule 3 â€“ Consecutive statements âˆ’ Just add â€¢ Rule 4 â€“ If-then-else âˆ’ Running time is never more than the time of the test (condition) plus the worst (ie, maximum) of the running times of the two branches âˆ’ Similarly for multiple/nested else statements Rules to compute running times ALG1(n) if (condition true) //O(1) else //O(n) And the following is the best part... we can ignore a whole lot of â€œclutterâ€ and focus solely on the dominant term â€¢ Rule 5 â€“ Remove constant multipliers âˆ’ E.g. O(2n) â†’ O(n) â€¢ Rule 6 â€“ Drop â€œnon-dominantsâ€ [lower-order terms] âˆ’ O(n2) + O(n) + O(1) â†’ O(n2) â€¢ Rule 7 â€“ Assume the WORST Rules to compute running times Letâ€™s have another look at the ARRAY-MAX example using a simpler approach â€¢ Knowing that we will simplify the final expression to focus on the asymptotically dominant expression anyway, we can â€œlook aheadâ€ and make the process of â€œcountingâ€ a lot simpler: ARRAY-MAX(A) max := A[0] for i = 1 to n-1 if A[i] > max then max := A[i] {increment counter i} return max SOME ANALYSIS EXAMPLES Loop â€“ Summing up squares SQUARES1(n) i := 0 sum := 0 while i < n increment i sum := sum + (i * i) return sum Is there a more efficient implementation ? The power of maths â€¢ Input: positive integer n â€¢ Output: the sum of the first n squares â€¢ T(n) = O(1) + O(1) = O(1) âˆ’ No loops! SQUARES2(n) sum := n * (n+1) * (2*n+1)/6 return sum Summation rule à· i=1 ð‘› ð‘–2 = ð‘›(ð‘› + 1)(2ð‘› + 1) 6 Operations O(1) O(1) Hmmm... â€¢ Input: positive integer n â€¢ Output: the integer part of the square root of n â€¢ How many times will this loop run? INT-SQRT1(n) i := 1 while (i*i ï‚£ n) increment i return i-1 Varying loop limitsVarying loop limits When the range of an inner loop is not constant, but depends (grows/shrinks) on a variable of an outer loop: â€¢ For our purposes of Big-O bounds: sufficient to work with the largest size that the range of that loop may take. â€¢ Reason: If you do an exact calculation, then apply the simplification rules, you will be left with this answer anyway! Recap â€¢ The asymptotic analysis of an algorithm determines the running time in big-O notation, and we focus on the worst case â€¢ To perform the asymptotic analysis âˆ’ We find the worst-case number of primitive operations executed, as a function of the input size âˆ’ We then simply this function to get itâ€™s big-O complexity â€¢ Example âˆ’ We determined algorithm ARRAY-MAX executes at most 7n âˆ’ 2 primitive operations âˆ’ We can, knowing the rules of simplification that isolate the dominant term, simplify the process of â€œcountingâ€ primitive operations. âˆ’ We say that algorithm ARRAY-MAX â€œruns in O(n) timeâ€ or, equivalently, â€has linear running timeâ€ â€¢ Since constant factors and lower-order terms are eventually ignored in the bigâ€“O notation, we can disregard them when counting primitive operations in the first place","libVersion":"0.3.2","langs":""}