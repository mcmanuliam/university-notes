{"path":"software engineering/year 2/semester 1/Systems Programming/Slides/2. Systems Programming/Intro to Concurrency.pdf","text":"Systems Programming – Part 2 Concurrent Systems Programming Dr Lauritz Thamsen lauritz.thamsen@glasgow.ac.uk https://lauritzthamsen.org Systems Programming Who Am I? 1 • Dr Lauritz Thamsen (he/him) • Lecturer in Computer Systems • Systems Research Section • Low-Carbon and Sustainable Computing theme • lauritz.thamsen@glasgow.ac.uk • https://lauritzthamsen.org • Office: S152 in Lilybank Gardens (2nd floor in the old part of the building, access through SAWB) • Office hours: 4 pm (right after the lectures) • Research Interests: • Distributed Computer Systems • Edge and Cloud Computing • Compute Resource Management • Low-Carbon Computing Systems Programming Acknowledgements • Lecture material • Nikos Ntarmos, Colin Perkins, Angelos Marnerides – Network and Operating Systems Essentials • Phil Trinder – Systems Programming (H) • Andreas Polze, Peter Tröger (Hasso Plattner Institute, University of Potsdam) – Parallel Programming • Odej Kao (Technische Universität Berlin) – Systems Programming • Labs and assessed exercise: Tom Wallis (previous lecturer of the course) – Systems Programming (GA) (2020-2022) • Advise from: Matthew Barr, Syed Waqar Nabi, Yehia Elkathib 2Systems Programming Topics of Part 2 of SP(GA) • Intro to Concurrency (with Processes and Threads) • Process/Thread Synchronisation • More on Process Management • Concurrency Beyond Threads & Limits of Scalability • Virtual Memory & Levels of Storage 3 pthreads labs CW2Exam Systems Programming Schedule of Part 2 of SP(GA) 4 Most of the days: first hour lecture, followed by a break, and then second hour lab time Date Day Lecture Focus Lab Focus 9 Mon* 2.1-2: Intro to Concurrency & pthreads No lab (double lecture) 10 Tue 2.3: Synchronisation (1/2) Lab 2-1: pthreads examples 11 Wed 2.3: Synchronisation (2/2) Lab 2-2: More pthreads examples 12 Thu 2.4: More on Process Management Lab 2-3: pthread synchronisation 13 Fri 2.5: Concurrency Beyond Threads Lab 2-4: More pthread synchronisation 16 Mon 2.6: Virtual Memory & Caching (1/2) Lab-AE2: Lab time to work on AE 2 17 Tue 2.6: Virtual Memory & Caching (2/2) Lab-AE2: More lab time to work on AE 2 18 Wed Full Course Revision Exam preparation * Release of Coursework 2, including an overview video Systems Programming Coursework 2 5 • Worth 15% of the overall grade • Release: 9 December 2022, 4 pm • Due: 19 December 2022, 10 pm • Asking you to reflect on the concurrency and memory management features of three programming languages in the context of a hypothetical project scenario Page 1 Systems Programming Textbook of Part 2 of SP(GA) 6 • Much of what we will cover on processes, synchronisation, and virtual memory is covered in: Silberschatz, Galvin, and Gagne, Operating System Concepts, (9th edition) • It is a widely used intro textbook on operating systems, and will provide additional context and details Let’s Make a Start! 7Systems Programming Lecture Outline • Motivation for Concurrent Systems Programming • Concurrency vs. Parallelism • Classes of Concurrent Programs • Processes and Threads • Second lecture today: Lecture 2.2 – Intro to POSIX Threads Systems Programming Why Concurrent Systems Programming? 9 • Sharing resources and improving responsiveness • Using parallel hardware and speeding executions up • Scaling to do more of the same P1 P2 P1 T1 T2T3 T1-2 T1-4T1-1 T1-3 … Why Concurrent Systems Programming? Because: Today’s Hardware Systems Programming Recap: Computer Architecture • Today, computer architecture is largely standardised, at a high level of abstraction, on the von Neumann Architecture Arithmetic/ Logic Unit (ALU) Control Unit Memory (Instructions and Data) Input Output 11Systems Programming Recap: The Processor • Often also referred to as the Central Processing Unit (CPU) • ALU + Control Unit • CPU-internal and high-speed cache memory 12 12 Arithmetic/ Logic Unit (ALU) Control Unit Cache Memory Input Output Processor Memory (Instructions and Data) 12Systems Programming Multi-Core Processors • We see a renewed interest in parallel architectures • Faster, although • They complicate system software • Not always possible to hide the complexity from application software (esp. to take full advantage of hardware) Arithmetic Unit Control Unit Memory Input Output Arithmetic Unit Arithmetic Unit Arithmetic/ Logic Unit, or “Core” 13Systems Programming Another Multi-Core Architectures Main memory Arithmetic / Logic Unit (ALU) Control Unit Level 1 Cache Memory Core Arithmetic /Logic Unit (ALU) Control Unit Level 1 Cache Memory Core Level 2 Cache Memory 14Systems Programming Coarser-Grained Parallelism: Clusters • We can also increase performance by linking computers using high-speed networks • Idea of servers • They don’t all need screens, etc. • Applications run across the cluster (ideally) • Again: some applications cannot easily be decomposed in this way 15 Concurrency? Parallelism? Our Terminology! Systems Programming Concurrency vs Parallelism 17 … when people hear the word concurrency they often think of parallelism, a related but quite distinct concept. In programming, concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Andrew Gerrand on the Go blog Systems Programming Concurrency vs Parallelism 18 • Concurrency • Supports to have two or more actions in progress at the same time • Demands scheduling and synchronisation • Classical operating system responsibility (resource sharing for better utilization of CPU, memory, network, ...) • Parallelism • Supports to have two or more actions executing simultaneously • Demands parallel hardware, concurrency support, (and communication) • Programming model relates to available hardware and communication Classes of Concurrent Programs 19Systems Programming Two Classes of Concurrent Programs 20 There are many different methods to enable concurrent programs We distinguish two main classes: • Shared memory locations are read and modified to communicate between concurrent components. Requires synchronisation to ensure that communication happens safely. We will mainly look at explicit concurrent systems programming with threads and use shared memory communication. • Message passing tends to be far easier to reason about than shared- memory concurrency, and is typically considered a more robust form of concurrent programming. Examples of message-passing systems are the actor model implemented in Erlang or CSP-style communication in Go. Covered in the Distributed and Parallel Technologies H/M course Systems Programming Communication Models • Two models of Inter Process Communication (IPC) a) Message passing b) Shared memory Processes and Threads Systems Programming Processes • A process is a program in execution that needs resources • A program is a passive entity (executable file) • A program becomes a process when its executable is loaded into memory • Each process has its own memory address space • Multiple processes can be executed simultaneously • One program can involve several processes • One program can be started multiple times, each time in one or more additional processes 23Systems Programming Threads of ProcessesSource: A. Silberschatz, “Operating System Concepts”, 9th Ed., 2012. 24Systems Programming Processes vs Threads 25 • Threads • A thread of execution is an independent sequence of program instructions • Multiple threads can be executed simultaneously • A process can have multiple threads sharing the same address space of the process, giving all threads access to the memory of the process (both heap and stacks, actually – but you should not access another thread’s stack) • We will use threads to implement concurrent programs • There is a program counter, specifying the location of the next instruction to execute, per thread • A single-threaded process has one thread with a program counter • A multi-threaded process has multiple program counters (one program counter per thread) Systems Programming Processes vs Threads • Threads of a process share memory and resources by default • Processes require specific communication mechanisms (shared memory, message passing) to be set up • Threads are faster/more economical to create • Each process has its own copy of the in-memory data, hence process creation means data duplication (at least of page tables) • Can have different schedulers for processes and threads • Depends on how threads are implemented (user threads vs kernel threads) • If a single thread in a process crashes, the whole process crashes as well • If a process dies, other processes are unaffected 26Systems Programming Application Example 27 • Many web browsers ran as a single process (some still do) • Google Chrome Browser is multi-process: • Browser process manages user interface, disk I/O, and network I/O • Renderer process • Process for plug-ins • Processes for web applications (“web workers”) Systems Programming Chrome on my Macbook 28Systems Programming … opening https://www.gla.ac.uk/ 29Systems Programming Summary: Advantages of Cooperating Tasks 30 • Cooperating processes/threads can affect or be affected by other processes, including sharing data • Advantages of cooperating processes? • Limited information sharing • Modularity • Convenience • Responsiveness, speed-up, scalability • And threads? • Responsiveness, speed-up, scalability Systems Programming Recommended Reading 31 • Silberschatz, Galvin, Gagne, Operating Systems Concepts, Sections 1.2-1.3, 3.1, and 4.1","libVersion":"0.3.2","langs":""}