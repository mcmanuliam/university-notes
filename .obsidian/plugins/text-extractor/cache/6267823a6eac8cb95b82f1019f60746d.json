{"path":"university/year 2/semester 1/Systems Programming/Slides/2. Systems Programming/Concurrency Beyond Threads & Limits of Scalability.pdf","text":"Systems Programming ‚Äì Part 2 Concurrent Systems Programming Dr Lauritz Thamsen lauritz.thamsen@glasgow.ac.uk https://lauritzthamsen.org Systems Programming Topics of Part 2 of SP(GA) ‚Ä¢ Intro to Concurrency (with Processes and Threads) ‚Ä¢ Process/Thread Synchronisation ‚Ä¢ More on Process Management (from an OS Perspective) ‚Ä¢ Concurrency Beyond Threads & Limits of Scalability ‚Ä¢ Virtual Memory & Levels of Storage 1Systems Programming Lecture Outline 2 ‚Ä¢ Intricacies of (Low-Level) Concurrency ‚Ä¢ (Higher-Level) Abstractions for Coordination ‚Ä¢ Options Within a Programming Language ‚Ä¢ Scalability and Limits of Scalability Systems Programming Low-Level Concurrency is Hard A concurrent program poses all of the challenges of sequential Computation: i.e. what to compute. In other words: A correct and efficient algorithm, using appropriate data structures, must be constructed. A concurrent program must also specify a correct and effective strategy for Coordination: i.e. how threads should cooperate. Systems Programming Concurrency Reflection 4 ‚Ä¢ Safely and correctly managing concurrent threads that share state is tricky ‚Ä¢ If you use locks, you must correctly lock, unlock, and wait to arrange safe access ‚Ä¢ You must avoid problems of Starvation: some thread never makes progress Livelock: threads run, but make no progress Deadlock: threads wait for each other to release locks Systems Programming Some Important Coordination Aspects 5 Partitioning: determining what parts of the computation should be evaluated separately ‚Ä¢ e.g. a thread to render each frame of a film Placement: determining where threads should be executed ‚Ä¢ e.g. allocate thread to the least busy core Communication: when to communicate and what data to send ‚Ä¢ e.g. film-rendering threads may hold two frames: one being processed and another ready to go next, as soon as the current frame is processed Synchronisation: ensuring threads can cooperate without interference ‚Ä¢ e.g. if two threads need to do work on the same frame, only one should change it at a time Systems Programming Lecture Outline 6 ‚Ä¢ Intricacies of (Low-Level) Concurrency ‚Ä¢ (Higher-Level) Abstractions for Coordination ‚Ä¢ Options Within a Programming Language ‚Ä¢ Scalability and Limits of Scalability Systems Programming Coordination Abstraction Levels 7 You have probably used several notations for specifying, designing & constructing computations but relatively few for coordination. Computations can be written in languages with different levels of abstraction, e.g. Low-level Mid-level High-Level Assembly C Java Python Prolog Haskell SQL Systems Programming Coordination Abstraction Levels 8 ‚Ä¢ Likewise, coordination can be written in languages with different levels of abstraction, e.g. Low-level Mid-level High-Level Mutexes Go OpenMP Semaphores Monitors (Java threads) C++ Threads Erlang Systems Programming High-Level Example: OpenMP 9 ‚Ä¢ ‚ÄúFork-Join‚Äù parallelism, repeatedly synchronizing independent threads (shared memory, but good practice not to share much state between threads) Main Thread #pragma omp parallel for for(int i = 1; i < 100; ++i) { // 100 independent computations } Systems Programming Lecture Outline 10 ‚Ä¢ Intricacies of (Low-Level) Concurrency ‚Ä¢ (Higher-Level) Abstractions for Coordination ‚Ä¢ Options Within a Programming Language ‚Ä¢ Scalability and Limits of Scalability Systems Programming Concurrent Coordination Options for Languages 11 A programming language may have several concurrency means, often competing, e.g. C++ has: Thread libraries, e.g. POSIX std threads ‚Ä¶ Java has: Thread libraries, e.g. POSIX Java threads Executors ‚Ä¶ Systems Programming ‚ÄúGreen Threads‚Äù 12 ‚Ä¢ Language-level concurrency: a thread that is managed by a runtime library or language virtual machine (== user threads), not natively by the operating system (== kernel threads) ‚Ä¢ Pioneered by Java (but later abandoned for ‚Äúreal‚Äù threads) ‚Ä¢ Widely used: Python‚Äôs Global Interpreter Lock, Fibers in Ruby (< 1.9), Goroutines in Go, Haskell, Erlang, ‚Ä¶ ‚Ä¢ Either ‚Äúcooperative multi-tasking‚Äù (== user threads have to yield voluntarily) or ‚Äúpreemptive multi-tasking‚Äù (== active user thread scheduling) Systems Programming User and Kernel Thread Mappings 13 In the end, only kernel threads run on processors, so we need to map user threads to kernel threads: Remember: We have threads and processes, giving us the flexibility to run also green-threaded programs in parallel (using multiple processes) Many-to-one (== Green Threads) Many-to-many (~ using a pool of kernel threads) One-to-one (== threading in C) Systems Programming ‚ÄúGreen Threads‚Äù 14 ‚Ä¢ Advantages ‚Ä¢ Lightweight: Creating, managing, and destroying kernel threads requires time and memory (i.e. reserving each thread‚Äôs stack) ‚Ä¢ Concurrency in a language ‚Äì independent of OS and hardware support for concurrency/parallelism ‚Ä¢ Easy-to-use concurrency (easy to have mutual exclusion, easier to reason about possible interleaving, especially with cooperative multi-tasking) ‚Ä¢ Disadvantage: Mapped to a single kernel thread ‚Üí can only offer concurrency, not parallelism ‚Ä¢ Cannot take advantage of parallel hardware ‚Ä¢ A single blocking user thread can block all others (if there is no preemption) ‚Ä¢ A user thread blocking on I/O can move all the user threads out-of-core Systems Programming Lecture Outline 15 ‚Ä¢ Intricacies of (Low-Level) Concurrency ‚Ä¢ (Higher-Level) Abstractions for Coordination ‚Ä¢ Options Within a Programming Language ‚Ä¢ Scalability and Limits of Scalability Systems Programming A Note on Performance ‚Ä¢ Which is the fastest? ‚Ä¢ ‚Ä¶ for winning on a racetrack? ‚Ä¢ ‚Ä¶ for transporting a family to the beach? ‚Ä¢ ‚Ä¶ for delivering five pizzas to different customers in a city? ‚Ä¢ Performance depends not only on the execution environment, but also on the workload! 5 x Systems Programming Scalability 17 ‚Ä¢ Performance: load a system can handle (at a certain scale) ‚Ä¢ Usually calculated as the mean, median, or x-percentile of load measurements (usually as throughput or latency) ‚Ä¢ Load: amount and properties of work, used to measure scalability ‚Ä¢ e.g. requests per second, read/write ratios, cache hit rates ‚Ä¢ Scalability: describes a system‚Äôs ability to cope with increased load: ‚ÄúThe system supports growths (in data volume, traffic volume, or complexity) with reasonable ways of dealing with it (e.g. more resources).‚Äù Systems Programming Types of Scaling 18 ‚Ä¢ Strong scaling: Fixed load, different resources ‚Ä¢ Best case: linear scalability (e.g. twice the VMs ‚Üí half the runtime) ‚Ä¢ There are usually diminishing returns from some point ‚Ä¢ Weak scaling: Scale resources and load proportionally ‚Ä¢ Can we use e.g. twice as many resources to handle twice the load (with the same performance? ‚Ä¢ Typically, this is less bounded Systems Programming Theoretical Limit: Amdahl‚Äôs Law ‚ÄúThe speedup of a program using multiple processors for parallel computing is limited by the sequential fraction of the program‚Äù ùëÜùë°ùëúùë°ùëéùëô = 1 1 ‚àí ùëù + ùëù ùë† ùëù: fraction of the program that benefits from parallelization ùë†: degree of parallelization (e.g. number of cores used) ‚Ä¢ Note: as ùë† approaches infinity, ùëù ùë† approaches 0, so ùëÜùë°ùëúùë°ùëéùëô becomes 1 1‚àíùëù ‚Ä¢ e.g. 50% parallel code, p = 0.5 ‚Üí speedup of at most 2 ‚Ä¢ e.g. 95% parallel code, p = 0.95 ‚Üí speedup of at most 20 Systems Programming Theoretical Limit: Amdahl‚Äôs Law 20 Amdahl s Law arallel r n m er r ess rs eed Systems Programming A Word of Caution 21 ‚Ä¢ Easiest way to reach good scalability? ‚Üí Have a poor baseline speed! ‚Ä¢ Scalability of System B is worse‚Ä¶ because of its better performance! Systems Programming Recommended Reading 22 ‚Ä¢ Recommended reading: ‚Ä¢ Silberschatz, Galvin, Gagne, Operating Systems Concepts, Sections 4.2-4 ‚Ä¢ Further reading recommendations (beyond this course): ‚Ä¢ C. Breshears, ‚ÄúThe Art of Concurrency: A Thread Monkey's Guide to Writing Parallel Applications‚Äù, 2009 ‚Ä¢ M. Kleppmann, ‚ÄúDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems‚Äù, 2017 ‚Ä¢ F. McSherry; M. Isard; D. G. Murray: ‚ÄúScalability! But at what COST?‚Äù, HotOS XI, 2015.","libVersion":"0.3.2","langs":""}