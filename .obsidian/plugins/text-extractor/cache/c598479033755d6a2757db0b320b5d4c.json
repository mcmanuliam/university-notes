{"path":"software engineering/year 2/semester 2/User Interaction/Slides/PDF Slides (1).pdf","text":"User Interaction COMPSCI2031 Dr Ilyena Hirskyj-Douglas ilyena.hirskyj-douglas@glasgow.ac.uk Recap: What we did last yesterday • Persona Task • Reading: None • Assignment: Ongoing! Deadlines soon. User Interaction Topics • HCI History and Introduction • Usability and Heuristics • Heuristic Evaluation and Human Cognition • Human Perception and Capabilities • Experimental Design & Variables Research • Personas and Scenarios • Surveys in HCI • Ethnography • Statical Methods • Theories in HCI • Models of Interaction • Large Scale and Mobile HCI • User-Centered Design • Ethics in User Testing • Revision & Example Exams Surveys in HCI Lecture 7 References and Readings Survey Research in HCI • Hedrik Müller, Aaron Sedley and Elizabeth Ferrall-Nunge URL: http://www.sunyoungkim.org/class/old/ede_sp17/readings/Week3_survey_Muller.pdf Survey: An important tool • Surveys are everywhere! They are used to make inferences about an entire population by gather information from a smaller subset (sample) of the population. • Widely used in Human-Computer Interaction (HCI) • to gather information about users' habits and interaction with technology, intent and motivation behind using a certain app • to get feedback from users • to quantitatively measure user satisfaction • to gather insights about users attitudes, perceptions, intents, habits, awareness's and experiences. Survey: An important tool • Surveys are fast and low cost, have broad reach • Can be done using various platforms and modalities • Online (website, emails) vs. Offline (pen & paper) • Big difference between quick-and-dirty surveys, and surveys that are properly planned, constructed and analysed. A well- planned survey can elicit useful insights while a poor one can be a waste of time. Why are surveys appropriate • How to design good surveys is an active research problem. • Overall, surveys are appropriate • When representing an entire population • To measure differences between groups of people • To identify changes in attitudes and experiences over time • Let’s take a closer look at what surveys can measure accurately Surveys are good for… • Measuring Attitude over time both accurately and objectively • Example: measure customer satisfaction with online banking. • Understanding Intent or motivation at a given time • Example: ‘Why do you use this website?’ will help understand intent • Can be deployed while a person is using an application (online intercept survey) minimising the risk of imperfect recall. • Quantify Task Success reliably • Example: Respondents perform a task, enter results of the task, and report their experience while performing the task. Surveys are good for… • Collect open-ended User Experience feedback • Example: ‘What would you change in the product?’ will elicit key product frustrations • Understanding User Characteristics to better serve their needs • Example: can collect users’ demographic information to understand difference • Understanding Interactions with Technologies • Example: What effect technology has on people across different demographics Surveys are good for… • Measuring Awareness of people of existing technology • Example: Whether low usage of a technical feature is due to lack of awareness or other factors • Making Comparisons users’ attitudes, perceptions, and experiences across user segments, time, geographies, and competing applications. When to avoid surveys Surveys are inexpensive and easy, but they cannot measure the following aspects of user interaction: • Precise Behaviour: Log data can often give more accurate info • Example: Which of the two features in an app people use first? Survey-based self-reporting is not good because user will struggle to recall such information. • Underlying Motivations: People often unsure/can’t explain why they prefer one thing over another. Better use ethnography or contextual inquiry. • Usability Evaluations: Surveys inappropriate for testing specific usability and understanding of tools and applications. Interview methods (usability studies) are better. Survey Pitfalls • Surveys also need to consider experimental design and confounding factors. • Multiple dependent variables in a single question. • Example: On a scale of 1 to 5, rate usability and how enjoyable your experience was. • Low completion rates, resulting from repetitive questions and poor usability and design. • Noisy data from bad questionnaire design (e.g., vague or ambiguous questions, biased questions) Using surveys with other methods Survey research may be especially beneficial when used in conjunction with other research methods. Using surveys with other methods • Qualitative studies help quantify specific observations through surveys. Often, up-front qualitative research may even be required to inform its content. • For example: • If a usability study uncovers a specific problem, a survey can quantify the frequency of that problem. • Survey can identify the range of frustrations or goals, followed by interviews and observational research to gain deeper insights. • May interview survey respondents to clarify responses, interview another pool of participants for comparison. How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Research Goals • Identify research goals and useful constructs • Example: How acceptable is the use of a VR headset during air travels? • Break it down to constructs: Seat position, Type of display, • Ask: How acceptable is it to use a VR headset in the aisle seat? How acceptable is it to use a seatback display in this aisle seat? How acceptable is it to use a VR headset in the window seat • Only crucial constructs: too many ‘nice to know’ questions will make a survey too long increasing drop-out rate. • Cognitive pretesting: Are respondents interpreting constructs as intended? How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Population and Sampling • A survey to understand the satisfaction of an application’s users. Population: everyone that uses the application Sampling frame: users that are actually reachable. May exclude past and anonymous users, and those who have opted out of survey. Sample: People who are invited to take a survey Respondent: Those who answer Population and Sampling • Survey respondents need to be recruited from the Sampling Frame • To ensure that the intended population is represented, you might need some inclusion criteria • For example, only respondents with over 20 hours gameplay can participate • Refer to earlier lectures for details about population, samples. How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Questionnaire Design • Poor design introduces bias and measurement error (deviation of respondent’s answers from their true values) • Mainly 2 categories of questions • Open-ended • Close-ended Questionnaire Design When to use open-ended questions? • Impossible to determine all possible answers in advance • List of options would be too long • Measuring quantities with natural metrics (when log data is unavailable), such as time, frequency, and length, e.g., “How many times do you use your tablet in a typical week?” • Qualitative aspects of user experience (see example in the previous slide) Questionnaire Design When to use closed-ended questions? • Only small number of possible answers and they are known • Rating an object on one dimension • Measuring quantities without natural metrics, such as importance, certainty, or degree, e.g., “How important is it to have your smartphone within reach 24 h a day?” • 1 to 5 scale: ‘Not at all’ to ‘Extremely important’ (unipolar) Types of Questions: • Single choice, multiple choice, ranking, rating questions • They have implications on how you analyse the data and what kind of statistics you can calculate Questionnaire Design Examples of Likert scale rating questions (Bipolar construct) Questionnaire Design: Bias • Each question must be carefully checked for bias • Bias introduced in one question can affect subsequent ones • Five common bias types • Satisficing (dishonesty, inattentive, or in a rush) • Acquiescence bias (tendency to agree) • Social desirability (viewed favourably) • Response order bias • Question order bias Questionnaire Design: Bias • Satisficing occurs when respondents use a suboptimal amount of cognitive effort to answer questions. • Fails to follow one of the 4 cognitive steps: comprehension of questions, retrieval of information from memory, judgement of the retrieved answer, mapping of the answer to the survey option • Weak satisficing: pick an answer that is suboptimal • Strong satisficing: pick an answer randomly • Satisfying occurs when cognitive ability or motivation to answer is low; Or, the question difficulty is high increasing cognitive load. Questionnaire Design: Bias • How to minimise satisfying? • Avoid complex questions and long questionnaires • Force choice by eliminating options such as “no opinion” • Avoid by offering even number of possible responses on scale • Avoid using same rating scale for a series of back-to-back questions • Ask to justify answer • Explain the importance of the survey to increase motivation Questionnaire Design • Acquiescence bias refers to respondents being more likely to agree in the presence of a binary design • Avoid yes/no questions, phrase questions neutrally • Social desirability bias refers to responses given because the respondent thinks it will be looked upon favourably • On a scale of 1-5, how important is climate change research? • Allow anonymous answer to minimise such bias Questionnaire Design • Response order bias is the tendency to choose options at the beginning or end. • Unrelated answer options should be randomly ordered across respondents • Rating scales should be ordered from negative to positive, with the most negative item first • Order of ordinal scales should be reversed randomly between respondents. • Question order bias is the effects of order of the questions. Each question in a survey has the potential to bias each subsequent question by priming respondents. • Order questions from broad to specific • Early questions should be easy and directly related • Group related questions to avoid context switching Types of Questions to Avoid • Broad Questions lack focus, provide noisy data and confuse respondents • Leading questions influence respondents to give a certain answer and add bias • Double-barrelled questions ask about multiple items while only allowing for a single response, resulting in less reliable and valid data. • Recall questions require the respondent to remember past attitudes and behaviours, leading to inaccurate and biased answers • Prediction questions ask respondents to anticipate future behaviours or attitudes, resulting in biased and inaccurate responses. Questionnaire Design Leverage existing questionnaires that are widely tested, validated, and accepted as standards, instead of starting one from scratch! How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Review and Testing • Early review and evaluation of a survey removes any confusion, identifies disconnect between researcher’s assumptions and how respondents will read, interpret, and answer questions. • Cognitive pretesting: A small set of potential respondents is invited to participate in an in-person interview where they are asked to take the survey while using the think-aloud protocol • Field testing: Run a pilot to review and evaluate survey before public launch • Checks realistic completion times • Assess success of sampling approach How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Implementation and Launch • After questions are finalised, the survey is ready to be fielded. • Respondents may be invited through e-mails to specifically named persons, intercept pop-up dialogs while using a product or a site (intercept survey), or links placed directly in an application • Many platforms and tools: Google Forms, Kinesis, LimeSurvey, SurveyMonkey, etc. • Decide based on functionality, cost, and ease of use • A researcher may want to customize the visual style or set up an automatic reporting dashboard. Implementation and Launch Tips: If working with a team, have a platform which multiple people can access is ideal Can do slow roll-out Some platforms (e.g., Microsoft Teams) you can limit to certain people e.g., within an organisation How to design a good survey? 1. Research Goals 2. Population and Sampling 3. Questionnaire Design 4. Review and Testing 5. Implementation and Launch 6. Data Analysis and Reporting Data Analysis After survey responses are collected, make sense of the data by following the steps below: 1. Preparing and exploring data 2. Analysing data 3. Reporting insights and results Step 1: Preparing data is important for removing duplicate responses, careless/inaccurate responses (speeders), straight-liners and other questionable patterns, missing data, outliers, inadequate open-ended responses etc. Data Analysis Step 2: Analysis of close-ended question requires • Descriptive statistics (moments, dispersion, etc) • Inferential statistics (correlation, regression, clustering etc.) • Hypothesis testing We will cover these topics in detail in Session 10 and 11. Data Analysis Step 2: Analysis of open-ended requires • ‘Coding’ - transform qualitative answers to quantitative metric • Prepare a full list of possible answers. Assign a ‘code’ (may be numbers or representative terms) to each answer • Assign each open-ended response to one of the ‘codes’ (rating) • Might require more than one raters while assigning the codes (normally inter-rater reliability). • Perform statistical analysis on the codes (e.g., frequency) Data Analysis Step 3: Reporting results and insights • Results of data analysis should be reported with the necessary statistical rigor (e.g., sample sizes, p -values, margins of error). • Report survey paradata (e.g., devices used, completion time, drop-off rates). • Describe original research goals and survey methodology. This explains the population being studied, sampling method, survey mode etc. • Include screenshots of the actual survey questions and explain techniques used to evaluate data • Discuss how the respondents compare to the overall population. • Discussion any potential sources of survey bias. Data Analysis Examples From Strangers to Friends, Kytö et al.: • Study looking at how people use AR profiles differently with strangers, friends or close friends. • Likert-based questionnaire (1..7 scale, 1 = strongly disagree, 7 = strongly agree). • This questionnaire covered the use of the profiles, interaction with other participants, and the interaction within the gathering itself. • “the participants who rated their relationship as a friend but not as a close friend (10 out of 20 participants) somewhat agreed with the statement ‘I found the other person’s private profile useful in get- ting to know him/her’ (M = 4.5, SD = 2.0).” • “Participants agreed with the statement ’My public profile helped to initiate conversations’ (M = 5.5, SD = 1.5) Questions? Comments? Concerns? Survey Task Part 1: As a group, create a short survey on one of the topics below: Option 1: Measure how frequently people play mobile games in the workplace. Option 2: Measure student's attitudes to using AI to write assignments. Your survey should contain five questions in total, including close-ended and open-ended questions. (15 mins) Part 2: Post your survey in another team's group chat. Fill in another group's survey by replying to the thread in another group with your answers. (5 mins) Part 3: Have a look at your answers to your survey and see if they correlate to what you expected and would help answer the research question given above. (10 mins) If you get stuck on types of questions, here are some examples of survey research papers: http://www.mkhamis.com/data/papers/ohagan2021ismar.pdf http://www.mkhamis.com/data/papers/ohagan2022imwut.pdf Survey Task: Class DiscussionUser Interaction Topics • HCI History and Introduction • Usability and Heuristics • Heuristic Evaluation and Human Cognition • Human Perception and Capabilities • Experimental Design & Variables Research • Personas and Scenarios • Surveys in HCI • Ethnography • Statical Methods • Theories in HCI • Models of Interaction • Large Scale and Mobile HCI • User-Centered Design • Ethics in User Testing • Revision & Example Exams Reading: Muller et al. Survey in HCI","libVersion":"0.3.2","langs":""}