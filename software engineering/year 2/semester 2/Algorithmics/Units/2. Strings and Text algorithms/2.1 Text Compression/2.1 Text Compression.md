#algo

# Problem
- given a string defined over an alphabet (e.g., ASCII or Unicode) 
- encode it efficiently into a smaller binary string using only 0s and 1s 

# When Do We want to Use It?

- communicating over a low-bandwidth channel, over a limited or poor wireless connection
- storing collections of large documents efficiently on a fixed capacity storage device

# Examples of Text Compression

- `compress`, `gzip` in unix `ZIP` util for windows…
- two main approaches: **statistical** and **dictionary**

# Compression Ration: `x/y`

- `x` is the size of compressed file and `y` is the size of the original file.
- e.g. measured in B, KB, MB, …
- compressing a 10MB file to 2MB would yield a compression ration of 2/10=0.2

# Huffman Encoding

The classical statistical method
- now mostly superseded in practice by more effective dictionary methods
- fixed ASCII replaced by `variable` length code for each character.
- every character is represent by a unique codeword (bit string)

## Based on a Huffman Tree (a Proper Binary tree)

- each char is a leaf
- codeword is given by a path to the node.

## Huffman Tree Construction

```
Character freguencies: 

Space  15
E      11
A      9
T      8
I      7
S      7
R      7
O      6
N      4
U      3
H      2
C      1
D      1
```

## To Create

- Add leaves, characters with their frequencies label the leaf nodes
- Next, while there is more than one parentless nodes
	- add new parent to nodes of smallest weight 
	- weight of new node equals sum of the weights of the child nodes
	
![[Screenshot 2025-03-04 at 10.25.23.png]]

Now we can follow down the paths to find the Huffman encoding, codeword for a character is given by the path from the root to the appropriate leaf (`left=0` and `right=1`)

![[Screenshot 2025-03-04 at 10.26.10.png]]

**prefix property**: no codeword is a prefix of another equivalently: no path to one character is a prefix of another (since characters are only found at leaves)

**Weighted path length (WPL)** of a tree $T − ∑ (weight)×(distance from root)$ where sum is over all leaf nodes − for the example tree: WPL equals: $7×4 + ..$

![[Screenshot 2025-03-04 at 10.29.24.png]]
$$7×4 + 1×7 + 1×7 + 2×6 +$$
![[Screenshot 2025-03-04 at 10.30.12.png]]

Huffman tree has minimum **WPL** over all binary trees with the given leaf weights 

- Huffman tree need not be unique (e.g., nodes>2 with min weight) 
- however, all Huffman trees for a given set of frequencies have same WPL 
- so what? 
- weighted path length (WPL) is the number of bits in compressed file 
	- bits = sum over chars (frequency of char × code length of char) 
	
- so a Huffman tree minimises this number

### Algorithmic Requirements

**Building the Huffman tree** 
- if the text length equals n and there are m distinct chars in text 
- `O(n)` time to find the frequencies 
- `O(mlog m)` time to construct the code, for example using a (min-) heap to store the parentless nodes and their weights 
- initially build a heap where nodes correspond to the m characters labelled by their frequencies, therefore takes `O(m)` time to build the heap 
	- one iteration takes `O(log m)` time: 
		- find and remove (`O(log m)`) two minimum weights 
		- then insert `(O(log m))` new weight (sum of minimum weights found) 
	- and there are m-1 iterations before the heap is empty 
		- each iteration decreases the size of the heap by 1
- so `O(n + mlog m)` overall 
- in fact, m is essentially a constant, so it is really `O(n)`

# LZW Compression

A popular dictionary-based method:
- the basis of `compress` and `gzip` in Unix, also used in `gif` and `tiff` formats.
- due to Abraham **L**empel, Jacob **Z**iv and Terry **W**elch.

Dictionary is a collection of strings
- each with a `codeword` that represents it
- the codeword is a bit pattern
- but it can be interpreted as a non-negative integer

Whenever a codeword is outputted during compression, what is written to the compressed file is the bit pattern
- using a number of bits determined by the current codeword length 
- at any point all codewords are the same length (no ambiguity)

## Key Question: how Many Bits Are in a Codeword?

in the most used version of the algorithm, this value changes as the compression (or decompression) algorithm proceeds.

- there is a current codeword length k 
- so there are exactly 2k distinct codewords available 
	- i.e., all possible bit-strings of length k
- this limits the size of the dictionary
- however the codeword length can be incremented when necessary 
	- thereby doubling the number of available codewords
- initial value of k should be large enough to encode all strings of length 1

## Let's Get into it

Okay first off when compressing, we have to create the dictionary, this gets a bit complicated but it's just a case of looping through each character and adding to a dictionary, if the character is already in the dictionary we can add in the current character and the one after (this should reduce tons of duplication).

```
dictionary: {
	A:0 
	C:1 
	G:2 
	T:3 
	GA:4 
	AC:5 
	CG:6 
	GAT:7 
	TA:8 
	ACG:9 
	GATA:10
}

codeword length: 4
```

![[Screenshot 2025-03-04 at 10.50.53.png]]

```java
set current text position i to 0;
initialise codeword length k (say to 8);
initialise the dictionary d;

while (the text t is not exhausted) {

	identify the longest string s, starting at position i of text t
	that is represented in the dictionary d;
	
	// there is such string, as all strings of length 1 are in d
	
	output codeword for the string s; // using k bits
	
	// move to the next position in the text 
	i += s.length(); // move forward by the length of string just encoded
	c = character at position i in t; // character in next position
	
	add string s+c to dictionary d, paired with next available codeword; 
	// this involves adding a new leaf node if d is represented by a trie
	// may have to increment the codeword length k to make this possible 
}
```